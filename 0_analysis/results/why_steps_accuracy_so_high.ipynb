{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../../../exports/dataset/results/balanced_agqa_preds/preds_psac.json', 'r') as f:\n",
    "#    preds = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../exports/dataset/results/steps_agqa_preds/preds_psac-2.json', 'r') as f:\n",
    "    preds = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'csv_q_id': 'YSKX3-1690',\n",
       " 'id': 0,\n",
       " 'question': 'After tidying up the object they were in front of, did they interact with some food?',\n",
       " 'prediction': 'Yes',\n",
       " 'answer': 'No',\n",
       " 'prediction_success': False}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: put into by video\n",
    "by_v = {}\n",
    "\n",
    "for p in preds:\n",
    "    v_id = p['csv_q_id'][:5]\n",
    "    if v_id not in by_v:\n",
    "        by_v[v_id] = []\n",
    "    by_v[v_id].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: go through each videoqs and get the num steps per question also the template\n",
    "for v in by_v:\n",
    "    ps = by_v[v]\n",
    "    \n",
    "    with open('../../../exports/dataset/balanced/test/%s.txt' % v, 'r') as f:\n",
    "        QA = json.load(f)\n",
    "    \n",
    "    for p in ps:\n",
    "        q_id = p['csv_q_id']\n",
    "        q = QA[q_id]\n",
    "        \n",
    "        p['steps'] = q['steps']\n",
    "        p['type'] = q['attributes']['type']\n",
    "        p['struct'] = q['attributes']['structural']\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: sort by template, answer, and num steps etc\n",
    "sort = {}\n",
    "sort_ans = {}\n",
    "sort_struct = {}\n",
    "sort_bo = {'binary': [], 'open': []}\n",
    "sort_steps = {}\n",
    "\n",
    "for v in by_v:\n",
    "    ps = by_v[v]\n",
    "    \n",
    "    for p in ps:\n",
    "        t = p['type']\n",
    "        s = p['steps']\n",
    "        st = p['struct']\n",
    "        a = p['answer']\n",
    "        \n",
    "        if t not in sort:\n",
    "            sort[t] = {}\n",
    "            \n",
    "        if s not in sort[t]:\n",
    "            sort[t][s] = []\n",
    "            \n",
    "        sort[t][s].append(p)\n",
    "        \n",
    "        if a not in sort_ans:\n",
    "            sort_ans[a] = []\n",
    "            \n",
    "        sort_ans[a].append(p)\n",
    "        \n",
    "        if st not in sort_struct:\n",
    "            sort_struct[st] = []\n",
    "            \n",
    "        sort_struct[st].append(p)\n",
    "        \n",
    "        \n",
    "        if st == 'query':\n",
    "            sort_bo['open'].append(p)\n",
    "        else:\n",
    "            sort_bo['binary'].append(p)\n",
    "            \n",
    "            \n",
    "        if t == 'actLast':\n",
    "            \n",
    "            if s not in sort_steps:\n",
    "                sort_steps[s] = []\n",
    "            sort_steps[s].append(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.9 77\n",
      "3 11.54 26\n",
      "4 13.79 29\n",
      "[{'steps': 1, 'acc': 3.9, 'number': 77}, {'steps': 3, 'acc': 11.54, 'number': 26}, {'steps': 4, 'acc': 13.79, 'number': 29}]\n"
     ]
    }
   ],
   "source": [
    "steps = []\n",
    "for i in sort_steps:\n",
    "    print(i, acc(sort_steps[i]), len(sort_steps[i]))\n",
    "    \n",
    "    x = {'steps': i, 'acc': acc(sort_steps[i]), 'number': len(sort_steps[i])}\n",
    "    steps.append(x)\n",
    "    \n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting accuracy of list\n",
    "\n",
    "def acc(ps):\n",
    "    cor = 0\n",
    "    for p in ps:\n",
    "        if p['prediction_success']:\n",
    "            cor += 1\n",
    "            \n",
    "    return round(cor / len(ps) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34.62 281105 [5, 4, 2, 6] objWhat\n",
      "50.0 44836 [7, 6, 5] objTime\n",
      "49.94 3258 [5] actLengthLongerVerify\n",
      "42.6 3258 [5] actLengthShorterChoose\n",
      "42.73 3258 [5] actLengthLongerChoose\n",
      "49.99 21708 [7, 6, 5] actTime\n",
      "49.78 22480 [6, 5, 3, 4, 7, 8] objFirstVerify\n",
      "2.33 558 [4, 3, 1] actFirst\n",
      "16.8 20892 [5, 2, 4, 6] objLast\n",
      "49.91 20796 [6, 4, 3, 5, 7, 8] objLastVerify\n",
      "50.0 21018 [5, 6] relTime\n",
      "17.44 30794 [2, 5, 4, 6] objFirst\n",
      "46.04 83862 [5, 3, 4, 7, 6, 8] objWhatChoose\n",
      "58.89 4143 [4, 3, 6, 5, 7, 8] objLastChoose\n",
      "49.93 21526 [4, 1, 3, 5] objExists\n",
      "49.98 20294 [4, 5, 3, 2, 1, 6] objRelExists\n",
      "49.9 21390 [4, 1, 3, 5] relExists\n",
      "49.98 17066 [7, 6, 5, 4, 8, 3] xorObjRelExists\n",
      "12.78 2879 [3, 4, 1, 5] objWhatGeneral\n",
      "49.94 17086 [6, 5, 4, 3, 7, 8] andObjRelExists\n",
      "44.82 2990 [4, 3, 5, 6, 7, 8] objFirstChoose\n",
      "2.67 262 [3, 4, 1] actWhatAfterAll\n",
      "4.15 289 [2] actLongest\n",
      "49.53 3176 [5] actLengthShorterVerify\n",
      "7.58 132 [1, 3, 4] actLast\n",
      "0.0 16 [2] actShortest\n",
      "11.11 135 [3, 4] actWhatBefore\n"
     ]
    }
   ],
   "source": [
    "for i in sort:\n",
    "    steps = list(sort[i].keys())\n",
    "    ps = []\n",
    "    for s in steps:\n",
    "        ps += sort[i][s]\n",
    "    print(acc(ps), len(ps), steps, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.98 verify 33612\n",
      "50.08 compare 27046\n",
      "38.47 choose 16229\n",
      "14.81 query 1249\n",
      "48.04 logic 11510\n"
     ]
    }
   ],
   "source": [
    "for st in sort_struct:\n",
    "    print(acc(sort_struct[st]), st, len(sort_struct[st]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.65 binary\n",
      "14.81 open\n"
     ]
    }
   ],
   "source": [
    "for i in sort_bo:\n",
    "    print(acc(sort_bo[i]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.19"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# things to investigaet\n",
    "\n",
    "Why is compare so high???\n",
    "Do I need to take out certain q_id???\n",
    "\n",
    "ohhhhhh\n",
    "ok try taking out q_ids in everything :( \n",
    "\n",
    "\n",
    "though thats only train right??? not test?\n",
    "\n",
    "\n",
    "So compare is being the issue again\n",
    "\n",
    "yeah its really just those three...\n",
    "\n",
    "\n",
    "but its ok for balanced.........\n",
    "UGH\n",
    "ok\n",
    "\n",
    "ok see on answers if its the same...\n",
    "\n",
    "OR WE do a similar question id removal process for steps. WIll need to see if that ends up changing the overall results and or meaning that we need to make new stwps issue. Im not sure though\n",
    "\n",
    "\n",
    "cause i would delete by program???? that might make fewer steps. idk if it would leave enoguh though tbd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.23 No\n",
      "93.53 Yes\n",
      "62.21 before\n",
      "20.41 window\n",
      "32.53 dish\n",
      "81.65 after\n",
      "22.45 paper\n",
      "65.32 table\n",
      "26.39 chair\n",
      "25.22 food\n",
      "47.17 mirror\n",
      "63.95 doorway\n",
      "39.6 closet\n",
      "0.0 holding a blanket\n",
      "0.0 undressing themselves\n",
      "44.18 bed\n",
      "51.4 doorknob\n",
      "27.16 clothes\n",
      "28.35 blanket\n",
      "13.48 shoe\n",
      "29.53 bag\n",
      "34.75 door\n",
      "6.8 broom\n",
      "52.88 laptop\n",
      "27.31 box\n",
      "15.87 refrigerator\n",
      "0.0 tidying something on the floor\n",
      "0.0 putting a broom somewhere\n",
      "10.0 standing up\n",
      "0.0 sitting in a bed\n",
      "44.44 floor\n",
      "0.0 opening a door\n",
      "1.75 picture\n",
      "23.18 phone\n",
      "0.0 turning off a light\n",
      "24.21 pillow\n",
      "11.76 television\n",
      "0.0 putting a blanket somewhere\n",
      "0.0 holding a phone\n",
      "0.0 grasping onto a doorknob\n",
      "42.67 light\n",
      "0.0 closing a closet\n",
      "0.0 watching something in a mirror\n",
      "0.0 dressing themselves\n",
      "1.43 medicine\n",
      "0.0 walking through a doorway\n",
      "0.0 throwing clothes somewhere\n",
      "0.0 going from standing to sitting\n",
      "0.0 putting some food somewhere\n",
      "0.0 taking food from somewhere\n",
      "0.0 fixing a doorknob\n",
      "0.0 smiling at something\n",
      "100.0 making some food\n",
      "0.0 throwing a blanket somewhere\n",
      "0.0 lying on a bed\n",
      "0.0 lying on the floor\n",
      "0.0 putting clothes somewhere\n",
      "0.0 putting a phone somewhere\n",
      "10.0 vacuum\n",
      "0.0 opening a window\n",
      "0.0 putting a cup somewhere\n",
      "0.0 holding a broom\n",
      "0.0 putting something on a table\n",
      "0.0 holding a vacuum\n",
      "0.0 tidying up a table\n",
      "0.0 holding a paper\n",
      "0.0 putting on a shoe\n",
      "0.0 fixing their hair\n",
      "0.0 watching television\n",
      "0.0 putting a laptop somewhere\n",
      "0.0 tidying up a blanket\n",
      "0.0 washing their hands\n",
      "0.0 holding a bag\n",
      "0.0 talking on a phone\n",
      "0.0 holding some clothes\n",
      "0.0 taking a bag from somewhere\n",
      "0.0 watching outside of a window\n",
      "0.0 sneezing somewhere\n",
      "0.0 putting a bag somewhere\n",
      "0.0 opening a box\n",
      "0.0 taking a phone from somewhere\n",
      "0.0 washing some clothes\n",
      "0.0 working on a book\n",
      "0.0 taking a pillow from somewhere\n",
      "0.0 holding a cup of something\n",
      "0.0 putting a box somewhere\n",
      "0.0 tidying up a closet\n",
      "0.0 opening a closet\n",
      "0.0 watching a picture\n",
      "0.0 holding some food\n",
      "0.0 opening a refrigerator\n",
      "0.0 turning on a light\n",
      "0.0 laughing at something\n",
      "0.0 closing a door\n",
      "0.0 washing something with a blanket\n"
     ]
    }
   ],
   "source": [
    "for i in sort_ans:\n",
    "    print(acc(sort_ans[i]), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = sort_ans['before']\n",
    "after = sort_ans['after']\n",
    "\n",
    "objTime = sort['objTime'][7]\n",
    "relTime = sort['relTime'][6]\n",
    "actTime = sort['actTime'][7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13495, 13551, -56)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(before), len(after), len(before) - len(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13729, 9998, 3319)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(objTime), len(relTime), len(actTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.85, 65.09, 64.09)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(objTime), acc(relTime), acc(actTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1\n",
    "\n",
    "Does the steps test have the deleted balanced? \n",
    "\n",
    "or does compo???\n",
    "\n",
    "\n",
    "ok so i think balanced and blind should be fine. but i think compo might be messed up. \n",
    "\n",
    "though balanced before q_ids? so im not sure\n",
    "\n",
    "\n",
    "The problem is that I dont know what happened in the deleting q_ids process. though maybe i also did it froma ll?\n",
    "\n",
    "So the things I did is i deleted questions, and i change question indexes\n",
    "\n",
    "maybe i look through balanced and balanced before q_id deletion? to see. also see whats different in dataset separating for compo steps\n",
    "\n",
    "\n",
    "\n",
    "Conclusion: I do think we deleted the training steps, so it should be fine.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def filenames(mypath):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6IL3.txt 134 12\n",
      "7T6MB.txt 271 22\n",
      "IJ3QB.txt 638 40\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../exports/dataset_separating for compo-steps/balanced before trying to smooth beforeafter/train/DSWBA.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-18de59182a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#with open('../../../exports/dataset/balanced-before-q_ids/train/%s' % v, 'r') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# with open('../../../exports/dataset_separating for compo-steps/balanced_combined/train/%s' % v, 'r') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../exports/dataset_separating for compo-steps/balanced before trying to smooth beforeafter/train/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mQA_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../exports/dataset_separating for compo-steps/balanced before trying to smooth beforeafter/train/DSWBA.txt'"
     ]
    }
   ],
   "source": [
    "identical = {True: 0, False: 0}\n",
    "\n",
    "diff_QA = None\n",
    "diff_QA_before = None\n",
    "\n",
    "v_ids = filenames('../../../exports/dataset/balanced/train/')\n",
    "for v in v_ids:\n",
    "    \n",
    "    with open('../../../exports/dataset/balanced/train/%s' % v, 'r') as f:\n",
    "        QA = json.load(f)\n",
    "    \n",
    "    #with open('../../../exports/dataset/balanced-before-q_ids/train/%s' % v, 'r') as f:\n",
    "    # with open('../../../exports/dataset_separating for compo-steps/balanced_combined/train/%s' % v, 'r') as f:\n",
    "    with open('../../../exports/dataset_separating for compo-steps/balanced before trying to smooth beforeafter/train/%s' % v, 'r') as f:\n",
    "        QA_before = json.load(f)\n",
    "    \n",
    "    \n",
    "    identical[len(QA) == len(QA_before)] += 1\n",
    "    \n",
    "    if len(QA) != len(QA_before):\n",
    "        diff_QA = QA\n",
    "        diff_QA_before = QA_before\n",
    "        print(v, len(QA), len(QA_before))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{True: 7786, False: 1}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7787"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141, 238)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(diff_QA), len(diff_QA_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1.5\n",
    "\n",
    "compare the q_ids of the balanced_csv omer had sent (tbh just download from the klone) to the balanced folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('../../../../../Desktop/train-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1441052"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(csv['key'].values)\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: put into by video\n",
    "by_v = {}\n",
    "\n",
    "for k in keys:\n",
    "    v_id = k[:5]\n",
    "    if v_id not in by_v:\n",
    "        by_v[v_id] = []\n",
    "    by_v[v_id].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in by_v:\n",
    "    \n",
    "    with open('../../../exports/dataset/balanced/train/%s.txt' % v, 'r') as f:\n",
    "        QA = json.load(f)\n",
    "        \n",
    "    if len(by_v[v]) != len(QA):\n",
    "        print(v, len(by_v[v]), len(QA))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7787, 7062)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_ids), len(by_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok \n",
    "\n",
    "so it does look like im right\n",
    "\n",
    "needed to train compo/steps on something else\n",
    "UGH\n",
    "i am stupid\n",
    "\n",
    "ok thats ok this is solveable\n",
    "\n",
    "\n",
    "\n",
    "# The GODDAMN plan\n",
    "\n",
    "1. [X] Make plan\n",
    "2. [X] crochet and youtube mind break\n",
    "3. [X] create a copy of the current balanced\n",
    "4. [X] Get rid of the problematic q_ids in the folder. I should probaly just use the CSV for this. \n",
    "5. [] Update Omer\n",
    "6. Re make the compo and steps csvs with these updated folders\n",
    "7. re-train compo and steps\n",
    "8. Do the inital analysis on eerything else, but with maybe getting rid of compare for now\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2\n",
    "\n",
    "\n",
    "see accruacy of this subset of q_ids in the normal balanced predition\n",
    "\n",
    "that way can know if its a training problem or a subset proble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../../../exports/dataset/results/balanced_agqa_preds/preds_psac.json', 'r') as f:\n",
    "#    preds = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
