{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import grammar\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Functions\n",
    "\n",
    "Just run through, nothing will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infile = open('../data/test_videos_stsg.pkl', 'rb')\n",
    "infile = open('../data/testing_camera_ready_stsgs.pkl', 'rb')\n",
    "test_stsgs = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "#infile = open('../data/test_videos_stsg.pkl', 'rb')\n",
    "infile = open('../data/training_camera_ready_stsgs.pkl', 'rb')\n",
    "train_stsgs = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snip(arr):\n",
    "    for i in range(len(arr)): \n",
    "        arr[i] = arr[i][:-1]\n",
    "def readLines(path):\n",
    "    file = open(path)\n",
    "    info = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "    #snip(info)\n",
    "    return info\n",
    "\n",
    "def loadPickle(path):\n",
    "    file = open(path, 'rb')\n",
    "    info = pickle.load(file)\n",
    "    file.close()\n",
    "    return info\n",
    "\n",
    "ENG = loadPickle('../data/eng.pkl')\n",
    "PP = loadPickle('../data/pres_part.pkl')\n",
    "IDX = loadPickle('../data/idx.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ2global = {\n",
    "    'objExists': ['exists'],\n",
    "    'objRelExists': ['exists', 'obj-rel'],\n",
    "    'relExists': ['exists'],\n",
    "    'actExists': ['exists'],\n",
    "    'andObjRelExists': ['exists', 'obj-rel'],\n",
    "    'xorObjRelExists': ['exists', 'obj-rel'],\n",
    "    'objWhatGeneral': [],\n",
    "    'objWhat': ['obj-rel'],\n",
    "    'objWhatChoose': ['obj-rel'],\n",
    "    'actWhatAfterAll': ['sequencing', 'action-recognition'],\n",
    "    'actWhatBefore': ['sequencing', 'action-recognition'],\n",
    "    'objFirst': ['superlative', 'obj-rel'],\n",
    "    'objFirstChoose': ['superlative', 'obj-rel'],\n",
    "    'objFirstVerify': ['superlative', 'obj-rel'],\n",
    "    'relFirst': ['superlative', 'obj-rel'],\n",
    "    'actFirst': ['superlative', 'action-recognition'],\n",
    "    'objLast': ['superlative', 'obj-rel'],\n",
    "    'objLastChoose': ['superlative', 'obj-rel'],\n",
    "    'objLastVerify': ['superlative', 'obj-rel'],\n",
    "    'relLast': ['superlative', 'obj-rel'],\n",
    "    'actLast': ['superlative', 'action-recognition'],\n",
    "    'actCount': ['count'],\n",
    "    'actCountChooseMore': ['count'],\n",
    "    'actCountChooseFewer': ['count'],\n",
    "    'actLengthLongerChoose': ['duration-comparison'],\n",
    "    'actLengthShorterChoose': ['duration-comparison'],\n",
    "    'actLengthLongerVerify': ['duration-comparison'],\n",
    "    'actLengthShorterVerify': ['duration-comparison'],\n",
    "    'actLongest': ['superlative', 'duration-comparison'],\n",
    "    'actShortest': ['superlative', 'duration-comparison'],\n",
    "    'actTime': ['sequencing'],\n",
    "    'relTime': ['rel-act'], # These should probably be sequencing too?\n",
    "    'objTime': ['obj-act'],\n",
    "}\n",
    "\n",
    "translate_type = {'objExists': 'Interaction', 'objRelExists': 'Interaction', 'relExists': 'Interaction', 'actExists': 'Interaction', 'andObjRelExists': 'Conjunction', 'xorObjRelExists': 'Conjunction',\n",
    "'objFirst': 'First/Last', 'objLast': 'First/Last', 'objFirstVerify': 'Equals', 'objLastVerify': 'Equals', 'objFirstChoose': 'Choose', 'objLastChoose': 'Choose', 'actFirst': 'Action Temporal Localization', 'actLast': 'Action Temporal Localization',\n",
    "'objWhatChoose': 'Choose', 'objWhatGeneral': 'Object', 'objWhat': 'Object', 'actWhatBefore': 'Action Temporal Localization', 'actWhatAfterAll': 'Action Temporal Localization',\n",
    "'actLengthLongerChoose': 'Choose', 'actLengthShorterChoose': 'Choose', 'actLengthLongerVerify': 'Equals', 'actLengthShorterVerify': 'Equals', 'actLongest': 'Longest/Shortest', 'actShortest': 'Longest/Shortest',\n",
    "'objTime': 'Choose', 'actTime': 'Choose', 'relTime': 'Choose'}\n",
    "\n",
    "\n",
    "\n",
    "translate_type1 = {\n",
    "    'Object Exists': 'Object Exists',\n",
    "    'Relation Exists': 'Relation Exists',\n",
    "    'Interaction': 'Interaction Temporal Localization',\n",
    "    'Object': 'Object', \n",
    "    'Action': 'Action',\n",
    "    'Interaction Temporal Localization': 'Interaction Temporal Localization',\n",
    "    'Interaction After': 'Interaction Temporal Localization',\n",
    "    'Interaction Before': 'Interaction Temporal Localization',\n",
    "    'Interaction While': 'Interaction Temporal Localization',\n",
    "    'Interaction Between': 'Interaction Temporal Localization',\n",
    "    'Exists Temporal Localization': 'Exists Temporal Localization',\n",
    "    'Exists After': 'Exists Temporal Localization',\n",
    "    'Exists Before': 'Exists Temporal Localization',\n",
    "    'Exists While': 'Exists Temporal Localization',\n",
    "    'Exists Between': 'Exists Temporal Localization',\n",
    "    'Object After': 'Object Temporal Localization',\n",
    "    'Object Before': 'Object Temporal Localization',\n",
    "    'Object While': 'Object Temporal Localization',\n",
    "    'Object Between': 'Object Temporal Localization',\n",
    "    'Object Temporal Localization': 'Object Temporal Localization',\n",
    "    \n",
    "    \n",
    "    'Indirect Action': 'Action Temporal Localization',\n",
    "    'Action After': 'Action Temporal Localization',\n",
    "    'Action Before': 'Action Temporal Localization',\n",
    "    'Action While': 'Action Temporal Localization',\n",
    "    'Action Between': 'Action Temporal Localization',\n",
    "    'First Object': 'First/Last',\n",
    "    'First Action': 'Action Temporal Localization',\n",
    "    'Last Object': 'First/Last',\n",
    "    'Last Action': 'Action Temporal Localization',\n",
    "    'Longest Action': 'Longest/Shortest',\n",
    "    'Shortest Action': 'Longest/Shortest',\n",
    "    'And': 'Conjunction', \n",
    "    'Xor': 'Conjunction',\n",
    "    'Conjunction': 'Conjunction',\n",
    "    'Choose': 'Choose',\n",
    "    'Longer Choose': 'Choose',#'Longer Action',\n",
    "    'Shorter Choose': 'Choose',#'Shorter Action',\n",
    "    'Longer Action': 'Choose',#'Longer Action',\n",
    "    'Shorter Action': 'Choose',#'Shorter Action',\n",
    "    'Equals': 'Equals',\n",
    "    'Object Equals': 'Equals',\n",
    "    'Action Equals': 'Equals',\n",
    "    'Order': 'Choose',\n",
    "    'Equals Action': 'Equals',\n",
    "    'Indirect Object': 'First/Last',\n",
    "    'nan': 'nan',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "translateType = translate_type\n",
    "translateType.update(translate_type1)\n",
    "\n",
    "\n",
    "\n",
    "def getTemplTranslation(x, q):\n",
    "    #x, q = merger.split('-')\n",
    "    if x in translateType:\n",
    "        y = translateType[x]\n",
    "        if y == 'Interaction Temporal Localization':\n",
    "            for word in ['before', 'after', 'while', 'between', 'Before', 'After', 'While', 'Between']:\n",
    "                if word in q:\n",
    "                    return y\n",
    "            return 'Interaction'\n",
    "        else:\n",
    "            return y\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translateAnsType(tp):\n",
    "    if tp == 'query':\n",
    "        return 'binary'\n",
    "    else:\n",
    "        return 'open'\n",
    "    \n",
    "def translateSemantic(tp):\n",
    "    if tp == 'objrel':\n",
    "        return 'relation'\n",
    "    elif tp in ['object', 'relation', 'action']:\n",
    "        return tp\n",
    "    else:\n",
    "        return 'none'\n",
    "    \n",
    "def translateGlobal(tp):\n",
    "    return templ2global[tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAnswerType(ans):\n",
    "    if ans == \"before\" or ans == \"after\":\n",
    "        return 'b/a'\n",
    "    if ans == \"Yes\" or ans == \"No\":\n",
    "        return 'y/n'\n",
    "    if type(ans) == int:\n",
    "        return 'count'\n",
    "    \n",
    "    return grammar.vType(IDX[ans])\n",
    "\n",
    "\n",
    "def formatAnswers(ans_list, ans_type):\n",
    "    if ans_type == 'count' or ans_type == 'b/a' or ans_type == 'y/n':\n",
    "        return ans_list\n",
    "    \n",
    "    new_list = []\n",
    "    for ans in ans_list:\n",
    "        if ans_type == 'objects':\n",
    "            ans = ENG[ans]\n",
    "        else:\n",
    "            ans = PP[ans]\n",
    "        new_list.append(ans)\n",
    "    return new_list\n",
    "\n",
    "        \n",
    "        \n",
    "def makeQuestion(stsgs, q):\n",
    "    if 'video' in q:\n",
    "        v_id = q['video']\n",
    "    else:\n",
    "        v_id = q['vid_id']\n",
    "    q_dict = {\n",
    "        'url': \"https://visualgenome.org/hits/agqa_videos?filename=\" + v_id,\n",
    "        'question': q['question'],\n",
    "        'gtanswer': q['answer'],\n",
    "        'objects': formatAnswers(stsgs[v_id]['obj_names'], 'objects'),\n",
    "        'actions': formatAnswers(stsgs[v_id]['act_names'], 'actions'),\n",
    "        'template': q['type'],\n",
    "        'ans_replaced': False\n",
    "        #'ans_type': translateAnsType(q['attributes']['structural']),\n",
    "        #'struct': q['attributes']['structural'],\n",
    "        #'sem': translateSemantic(q['attributes']['semantic']),\n",
    "        #'global': translateGlobal(q['attributes']['type']),\n",
    "        #'q_id': q['id']\n",
    "        \n",
    "    }\n",
    "   # print(\"||%s||\" % q['answer'])\n",
    "    ans_type = findAnswerType(q['answer'])\n",
    "        \n",
    "    if ans_type == 'objects':\n",
    "        pos_ans = stsgs[v_id]['obj_names']\n",
    "    elif ans_type == 'attention':\n",
    "        pos_ans = stsgs[v_id]['arel_names']\n",
    "    elif ans_type == 'contact':\n",
    "        pos_ans = stsgs[v_id]['crel_names']\n",
    "        for i in stsgs[v_id]['vrel_names']:\n",
    "            pos_ans.append(i)\n",
    "    elif ans_type == 'spatial':\n",
    "        pos_ans = stsgs[v_id]['srel_names']\n",
    "    elif ans_type == 'verb':\n",
    "        pos_ans = stsgs[v_id]['vrel_names']\n",
    "        for i in stsgs[v_id]['crel_names']:\n",
    "            pos_ans.append(i)\n",
    "    elif ans_type == 'actions':\n",
    "        pos_ans = stsgs[v_id]['act_names']\n",
    "    elif ans_type == 'count':\n",
    "        pos_ans = [0,1,2,3,4]\n",
    "    elif ans_type == 'b/a':\n",
    "        pos_ans = [\"Before\", \"After\"]\n",
    "    elif ans_type == 'y/n':\n",
    "        pos_ans = [\"Yes\", \"No\"]\n",
    "            \n",
    "    else:\n",
    "        print(\"NOT IN ANY CATEGORY\", q['answer'], ans_type)\n",
    "        \n",
    "    q_dict['possible_answers'] = formatAnswers(pos_ans, ans_type)\n",
    "        \n",
    "    return q_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendQuestion(by_comp_v, q):\n",
    "    a = q['answer']\n",
    "    if a is None or a == 'None':\n",
    "        return by_comp_v\n",
    "    \n",
    "    c = q['type']\n",
    "    question = q['question']\n",
    "    v_id = q['video']\n",
    "    if c not in by_comp_v:\n",
    "        by_comp_v[c] = {}\n",
    "\n",
    "    if v_id not in by_comp_v[c]:\n",
    "        by_comp_v[c][v_id] = {}\n",
    "\n",
    "    if question not in by_comp_v[c][v_id]:\n",
    "        by_comp_v[c][v_id][question] = []\n",
    "\n",
    "    by_comp_v[c][v_id][question].append(q)\n",
    "    \n",
    "    return by_comp_v\n",
    "\n",
    "\n",
    "def appendQuestionCSV(by_comp_v, q):\n",
    "    a = q['answer']\n",
    "    if a is None or a == 'None':\n",
    "        return by_comp_v\n",
    "    \n",
    "    c = q['amt_type']\n",
    "    question = q['question']\n",
    "    v_id = q['vid_id']\n",
    "    if c not in by_comp_v:\n",
    "        by_comp_v[c] = {}\n",
    "\n",
    "    if v_id not in by_comp_v[c]:\n",
    "        by_comp_v[c][v_id] = {}\n",
    "\n",
    "    if question not in by_comp_v[c][v_id]:\n",
    "        by_comp_v[c][v_id][question] = []\n",
    "\n",
    "    by_comp_v[c][v_id][question].append(q)\n",
    "    \n",
    "    return by_comp_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUsed(inpt):\n",
    "    used = []\n",
    "    for i in inpt:\n",
    "        item = '%s-%s' % (i['url'][-5:], i['question'])\n",
    "        used.append(item)\n",
    "    return used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sortQids(stsgs, used=[], look_at = []):\n",
    "    no_files = []\n",
    "    by_comp_v = {}\n",
    "    for i, v_id in enumerate(stsgs): # enumerate(['0A8ZT']):#\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        try:\n",
    "            #with open('../../../Desktop/qd/final_data/test_data/%s.json' % v_id, 'rb') as f:\n",
    "            with open('../question_decomposition/final_data/Test/%s.json' % v_id, 'rb') as f:\n",
    "                #with open('../exports/dataset/balanced/test/%s.txt' % v_id, 'rb') as f:\n",
    "                QA = json.load(f)\n",
    "        except:\n",
    "            no_files.append(v_id)\n",
    "            continue\n",
    "\n",
    "            \n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            q['video'] = v_id\n",
    "            #q['type'] = getTemplTranslation(q['type'])\n",
    "            q['type'] = getTemplTranslation(q['attributes']['type'])\n",
    "            if q['type'] not in look_at:\n",
    "                continue\n",
    "            \n",
    "            item = \"%s-%s\" % (v_id, q['question'])\n",
    "            if item not in used:\n",
    "                by_comp_v = appendQuestion(by_comp_v, q)\n",
    "                \n",
    "            continue\n",
    "            \n",
    "            subqs = QA[q_id]['subquestion']\n",
    "            for question in subqs:\n",
    "                q = subqs[question]\n",
    "                q['question'] = question\n",
    "                q['video'] = v_id\n",
    "                \n",
    "                item = \"%s-%s\" % (v_id, q['question'])\n",
    "                if item not in used:\n",
    "                    by_comp_v = appendQuestion(by_comp_v, q)\n",
    "\n",
    "    return by_comp_v, no_files\n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "def sortQidsCSV(stsgs, used=[], look_at = []):\n",
    "    no_files = []\n",
    "    by_comp_v = {}\n",
    "\n",
    "    csv = pd.read_csv('../question_decomposition/final_data/data/Test_csvs/Test.csv')\n",
    "    \n",
    "    print(\"loadded csv\")\n",
    "    csv = csv[csv['answer'].isna() == False]\n",
    "    print('dropped nan')\n",
    "    print('columns', csv.columns)\n",
    "    csv['amt_type'] = csv.apply(lambda x: getTemplTranslation(x['type'], x['question']), axis=1)\n",
    "    print('got amt type')\n",
    "    qs = csv.to_dict('records')\n",
    "    print('translated to quesitons')\n",
    "    \n",
    "            \n",
    "    for i, q in enumerate(qs):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)\n",
    "        v_id = q['vid_id']\n",
    "        \n",
    "        if v_id not in stsgs:\n",
    "            continue\n",
    "        \n",
    "        tp = q['amt_type']\n",
    "        #q['type'] = getTemplTranslation(q['type'])\n",
    "        #q['type'] = getTemplTranslation(q['attributes']['type'])\n",
    "        if tp not in look_at:\n",
    "            continue\n",
    "\n",
    "        item = \"%s-%s\" % (v_id, q['question'])\n",
    "        if item not in used:\n",
    "            by_comp_v = appendQuestionCSV(by_comp_v, q)\n",
    "\n",
    "    return by_comp_v, no_files\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get an approximately equal number of questions\n",
    "def getQs(by_comp, num):\n",
    "    num_cat = len(by_comp)\n",
    "    num_per_cat = math.floor(num / num_cat)\n",
    "    leftovers = num - (num_per_cat * num_cat)\n",
    "    selected = {}\n",
    "    \n",
    "    for c in by_comp:\n",
    "        qs = []\n",
    "        for v in by_comp[c]:\n",
    "            for q in by_comp[c][v]:\n",
    "                qs += random.sample(by_comp[c][v][q], k=1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #qs = by_comp[c]\n",
    "        if len(qs) < num_per_cat:\n",
    "            leftovers += num_per_cat - len(qs)\n",
    "            num_sample = len(qs)\n",
    "        else:\n",
    "            num_sample = num_per_cat\n",
    "            \n",
    "        selected[c] = random.sample(qs, k=num_sample)\n",
    "    \n",
    "    \n",
    "    print(\"Getting %s per category, with %s leftovers\" % (num_per_cat, leftovers))\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getRemainingPerComp(goal, curr):\n",
    "    remaining = {}\n",
    "    \n",
    "    for c in curr: \n",
    "        curr_num = curr[c]\n",
    "        \n",
    "        if curr_num < goal:\n",
    "            remaining[c] = goal - curr_num\n",
    "        else:\n",
    "            remaining[c] = 0\n",
    "            \n",
    "        if curr[c] + remaining[c] < goal:\n",
    "            print(\"Problem!\")\n",
    "            \n",
    "    return remaining\n",
    "\n",
    "\n",
    "# Get a specified number of questions per category\n",
    "def fillQs(by_comp, num, num_by_comp):\n",
    "    num_cat = len(by_comp)\n",
    "    #num_per_cat = math.floor(num / num_cat)\n",
    "    #leftovers = num - (num_per_cat * num_cat)\n",
    "    selected = {}\n",
    "    \n",
    "    for c in by_comp:\n",
    "        qs = []\n",
    "        num_per_cat = num_by_comp[c]\n",
    "        for v in by_comp[c]:\n",
    "            for q in by_comp[c][v]:\n",
    "                qs += random.sample(by_comp[c][v][q], k=1)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #qs = by_comp[c]\n",
    "        if len(qs) < num_per_cat:\n",
    "            print(\"not enough for category\", c, \"we want\", num_per_cat, 'but only have', len(qs))\n",
    "            #leftovers += num_per_cat - len(qs)\n",
    "            num_sample = len(qs)\n",
    "        else:\n",
    "            num_sample = num_per_cat\n",
    "            \n",
    "        selected[c] = random.sample(qs, k=num_sample)\n",
    "    \n",
    "    \n",
    "    #print(\"Getting %s per category, with %s leftovers\" % (num_per_cat, leftovers))\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 1: Generate question list for overall number of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n"
     ]
    }
   ],
   "source": [
    "by_comp, no_files = sortQids(train_stsgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "for i in by_comp:\n",
    "    for j in by_comp[i]:\n",
    "        for k in by_comp[i][j]:\n",
    "            all_questions.append(by_comp[i][j][k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "allqs = len(all_questions)\n",
    "\n",
    "notAGQA = 2329261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4004789735132358"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notAGQA / allqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_comp, no_files = sortQids(train_stsgs)\n",
    "print('There are no questions for %s videos' % len(no_files))\n",
    "sampled = getQs(by_comp, 200)\n",
    "\n",
    "qs = []\n",
    "\n",
    "for cat in sampled:\n",
    "    print(cat)\n",
    "    for q in sampled[cat]:\n",
    "        qs.append(makeQuestion(test_stsgs, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 2: Fill in so X per category\n",
    "\n",
    "- need to not include questions from before!!!\n",
    "    - make a list of \"v_id-question\" strings and then input\n",
    "- find the number remaining and sample those\n",
    "- ensure is divisible by 5 (Fill in with false negs and whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_155_falseneg.txt', 'rb') as f:\n",
    "    inpt1 = json.load(f)\n",
    "    \n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_45_falseneg.txt', 'rb') as f:\n",
    "    inpt2 = json.load(f)\n",
    "    \n",
    "\n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching10.txt', 'rb') as f:\n",
    "    inpt3 = json.load(f)\n",
    "    \n",
    "\n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching15.txt', 'rb') as f:\n",
    "    inpt4 = json.load(f)\n",
    "    \n",
    "    \n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching25.txt', 'rb') as f:\n",
    "    inpt5 = json.load(f)\n",
    "    \n",
    "\n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching40.txt', 'rb') as f:\n",
    "    inpt6 = json.load(f)\n",
    "    \n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching25-interact.txt', 'rb') as f:\n",
    "    inpt7 = json.load(f)\n",
    "    \n",
    "    \n",
    "inpt = []\n",
    "inpt += inpt1\n",
    "inpt += inpt2\n",
    "inpt += inpt3\n",
    "inpt += inpt4\n",
    "inpt += inpt5\n",
    "inpt += inpt6\n",
    "inpt += inpt7\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/analysis/accPerCat.txt', 'rb') as f:\n",
    "    acc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned = ['Indirect Action', 'Object', 'Shorter Action',\n",
    "         'Choose', 'Indirect Object', 'Equals', 'Conjunction']\n",
    "\n",
    "v_ids = []\n",
    "\n",
    "mypath_frame = '../AMT/easyturk/easyturk/videos/%s/'\n",
    "\n",
    "paths = ['11-8', '11-10', '11-12', '11-13', '11-13-2', '11-14', '11-15']\n",
    "\n",
    "for path in paths:\n",
    "    mypath = mypath_frame % path\n",
    "\n",
    "    v_ids += [f[:5] for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "\n",
    "v_ids = list(set(v_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "655"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_old = {'Indirect Action': 20,\n",
    " 'Shorter Action': 30,\n",
    " 'Choose': 26,\n",
    " 'Indirect Object': 29,\n",
    " 'Object': 23,\n",
    " 'Equals': 28,\n",
    " 'Conjunction': 28,\n",
    "}\n",
    "\n",
    "remaining = {\n",
    "    'Longer Action': 15,\n",
    "    #'Temporal Localization': 15,\n",
    "    'Interaction': 10,\n",
    "    'Shortest Action': 15,\n",
    "    'Exists Temporal Localization': 15,\n",
    "    'Order': 9,\n",
    "    'Relation Exists': 15,\n",
    "    'Longest Action': 15,\n",
    "    'Equals Action': 13,\n",
    "    'Object Exists': 15,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "remaining = {\n",
    "    'Interaction': 24,\n",
    "    'Interaction Temporal Localization': 1 # Maybe more to see? idk \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "remaining = {\n",
    "    'Interaction': 15,\n",
    "    #'Interaction Temporal Localization': 1 # Maybe more to see? idk \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadded csv\n",
      "dropped nan\n",
      "columns Index(['type', 'program', 'answer', 'key', 'question', 'vid_id', 'gif_id',\n",
      "       'desciption'],\n",
      "      dtype='object')\n",
      "got amt type\n",
      "translated to quesitons\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "There are no questions for 0 videos\n"
     ]
    }
   ],
   "source": [
    "used = getUsed(inpt)\n",
    "\n",
    "by_comp, no_files = sortQidsCSV(v_ids, used, list(remaining))\n",
    "print('There are no questions for %s videos' % len(no_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction\n"
     ]
    }
   ],
   "source": [
    "for i in by_comp:\n",
    "    print(i)\n",
    "    for j in by_comp[i]:\n",
    "        if j not in v_ids:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in remaining:\n",
    "    if i not in by_comp:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = acc['Total']\n",
    "goal = 15\n",
    "#remaining = getRemainingPerComp(goal, curr)\n",
    "sampled = fillQs(by_comp, goal, remaining)#getQs(by_comp, 200)\n",
    "\n",
    "qs = []\n",
    "\n",
    "for cat in sampled:\n",
    "    for q in sampled[cat]:\n",
    "        qs.append(makeQuestion(test_stsgs, q))\n",
    "        \n",
    "        \n",
    "random.shuffle(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the person smiling at a book?\n",
      "Is the person going from standing to sitting?\n",
      "Is the person holding the last object that they are on the side of?\n",
      "Is the person eating some food?\n",
      "Did they hold the object they were on the side of last?\n",
      "In the video, was the person watching something?\n",
      "Is the person taking a cup from somewhere?\n",
      "Is the person taking shoes from somewhere?\n",
      "In the video, had they put a pillow somewhere?\n",
      "Is the person putting a laptop somewhere?\n",
      "Is the person eating the last object that they are putting down?\n",
      "Is the person touching the last object that they are in front of?\n",
      "Is the person taking a pillow from somewhere?\n",
      "Is the person sitting the first object that they are beneath?\n",
      "Is the person sitting the first object that they are in front of?\n"
     ]
    }
   ],
   "source": [
    "x = [q['question'] for q in qs]\n",
    "\n",
    "for i in x:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction 15\n"
     ]
    }
   ],
   "source": [
    "for i in sampled:\n",
    "    print(i, len(sampled[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have to add in order questions for choose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadded csv\n",
      "dropped nan\n",
      "got amt type\n"
     ]
    }
   ],
   "source": [
    "\n",
    "csv = pd.read_csv('../question_decomposition/final_data/data/Test_csvs/Test.csv')\n",
    "\n",
    "print(\"loadded csv\")\n",
    "csv = csv[csv['answer'].isna() == False]\n",
    "print('dropped nan')\n",
    "csv['amt_type'] = csv['type'].apply(getTemplTranslation)\n",
    "csv = csv[csv['answer'].isin(['before', 'after'])]\n",
    "before = csv[csv['answer'] == 'before']\n",
    "after = csv[csv['answer'] == 'after']\n",
    "print('got amt type')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40708, 9), (40718, 9), (81426, 9))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before.shape, after.shape, csv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = csv.sample(n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_qs = x.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_qs = [makeQuestion(test_stsgs, q) for q in ba_qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs += ba_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in remaining:\n",
    "    tot += remaining[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://visualgenome.org/hits/agqa_videos?filename=7RXMM',\n",
       " 'question': 'What were they doing for the shortest amount of time?',\n",
       " 'gtanswer': 'sitting in a chair',\n",
       " 'objects': ['shoe', 'clothes', 'chair', 'mirror'],\n",
       " 'actions': ['holding a shoe',\n",
       "  'sitting in a chair',\n",
       "  'watching something in a mirror',\n",
       "  'standing up',\n",
       "  'holding some clothes'],\n",
       " 'template': 'Shortest Action',\n",
       " 'ans_replaced': False,\n",
       " 'possible_answers': ['holding a shoe',\n",
       "  'sitting in a chair',\n",
       "  'watching something in a mirror',\n",
       "  'standing up',\n",
       "  'holding some clothes']}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get v_ids to upload videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_ids = set()\n",
    "for q in qs:\n",
    "    v_ids.add(q['url'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(v_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S4P5J'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(v_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '../Charades_v1_480/%s.mp4'\n",
    "dest_path = '../AMT/easyturk/easyturk/videos/11-15/%s.mp4'\n",
    "\n",
    "for v_id in v_ids:\n",
    "    src_file = src_path % v_id\n",
    "    dest_file = dest_path % v_id\n",
    "    shutil.copyfile(src_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add false negatives\n",
    "\n",
    "### Used: \n",
    "- valid false\n",
    "- not_included 0-25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk copy/easyturk/data/dataset/false_negatives52.txt', 'rb') as f:\n",
    "    false = json.load(f)\n",
    "    \n",
    "    \n",
    "url_path = \"https://visualgenome.org/hits/agqa_videos?filename=\"\n",
    "\n",
    "valid_false = []\n",
    "not_included = []\n",
    "for i in false:\n",
    "    if not i['ans_replaced']:\n",
    "        print(i)\n",
    "        \n",
    "    \n",
    "    v_id = i['url'][-9:-4]\n",
    "    i['old_url'] = i['url']\n",
    "    new_url = url_path + v_id\n",
    "    i['url'] = new_url\n",
    "    \n",
    "    v_id = i['url'][-9:-4]\n",
    "    if v_id in v_ids:\n",
    "        valid_false.append(i)\n",
    "    else:\n",
    "        not_included.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_qs = qs#+ not_included[25:28]\n",
    "len(all_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_so_far = all_qs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(all_qs)\n",
    "for i, q in enumerate(all_qs):\n",
    "    if q['ans_replaced']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in qs: \n",
    "    v = q['url'][-5:]\n",
    "    if v not in v_ids:\n",
    "        print(v)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_so_far' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-d0ca8927e001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_so_far\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_so_far' is not defined"
     ]
    }
   ],
   "source": [
    "len(best_so_far) / 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open('../AMT/easyturk/easyturk/data/dataset/input_interact15.txt', 'w+') as f:\n",
    "        json.dump(best_so_far, f)\n",
    "else:\n",
    "    print(\"NOT SAVING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in not_included[15:19]:\n",
    "    print(i['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../AMT/easyturk/easyturk/data/dataset/input_reaching25-interact.txt', 'rb') as f:\n",
    "    inpt = json.load(f)\n",
    "    \n",
    "\n",
    "with open('../AMT/easyturk/easyturk/results/%s.txt' % 'results-fill25-second2', 'rb') as infile:\n",
    "    results = pickle.load(infile)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadResults(paths):\n",
    "    results = {}\n",
    "    for p in paths:\n",
    "        with open('../AMT/easyturk/easyturk/results/%s.txt' % p, 'rb') as infile:\n",
    "            new_results = pickle.load(infile)\n",
    "        results.update(new_results)\n",
    "        \n",
    "    dicts = []\n",
    "    for i in results:\n",
    "        for j in results[i]:\n",
    "            dicts += j['output']\n",
    "            \n",
    "\n",
    "    df = pd.DataFrame(dicts)\n",
    "    \n",
    "    data = df[df['ans_replaced'] == False]\n",
    "    ans_rep = df[df['ans_replaced']]\n",
    "    \n",
    "    return df, data, ans_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inpt), len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    'results-fill25-second2',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "df, data, ans_rep = loadResults(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_df = pd.DataFrame(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_qs = [i['question'] for i in inpt]\n",
    "r_qs = data['question'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_yet = []\n",
    "\n",
    "for i in i_qs:\n",
    "    if i not in r_qs:\n",
    "        not_yet.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is the person sitting the first object that they are leaning on?',\n",
       " 'Was fixing a door something a person did?',\n",
       " 'Is the person holding the last object that they are taking?',\n",
       " 'Is the person holding the last object that they are putting down?',\n",
       " 'Is the person touching the first object that they are taking?',\n",
       " 'In the video, did they lean on anything?',\n",
       " 'Was a blanket one of the things they were interacting with while putting the object they were behind first somewhere?',\n",
       " 'Is the person watching the last object that they are on the side of?',\n",
       " 'Is the person taking the first object that they are on the side of?',\n",
       " 'Was a broom one of the things they were interacting with?',\n",
       " 'Is the person touching a door?',\n",
       " 'Is the person touching the last object that they are in front of?',\n",
       " 'Is the person holding the last object that they are taking?',\n",
       " 'Is the person touching the object that they are opening?',\n",
       " 'Had the person dressed anything?']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for immediate fixing\n",
    "\n",
    "1. Change url of existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = \"http://bluelagoon.stanford.edu:5000/?filename=\"\n",
    "\n",
    "for q in qs:\n",
    "    v_id = q['url'][-9:-4]\n",
    "    new_url = url_path + v_id\n",
    "    q['old_url'] = q['url']\n",
    "    q['url'] = new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_200_new_url_EXED.txt', 'w+') as f:\n",
    "    json.dump(qs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_200_vids.txt', 'w+') as f:\n",
    "    json.dump(list(v_ids), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_path = \"https://visualgenome.org/hits/agqa_videos?filename=\"\n",
    "\n",
    "for q in qs:\n",
    "    v_id = q['url'][-5:]\n",
    "    new_url = url_path + v_id\n",
    "    q['old_bad_url'] = q['url']\n",
    "    q['url'] = new_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in qs:\n",
    "    q['ans_replaced'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_200_new_url3.txt', 'w+') as f:\n",
    "    json.dump(qs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for v in v_ids:\n",
    "        with open('../exports/dataset/balanced/test/%s.txt' % v, 'rb') as f:\n",
    "            QA = json.load(f)\n",
    "\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "\n",
    "            if q['attributes']['type'] == 'objRelExists' and q['answer'] == 'No':\n",
    "                if q['metrics']['indirects'].count(True) == 0:\n",
    "                    print(q_id, q['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = test_stsgs['F7TG5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ENG[i] for i in s['act_names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/results/results-first4.txt', 'rb') as infile:\n",
    "    results = pickle.load(infile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = []\n",
    "for i in results:\n",
    "    for j in results[i]:\n",
    "        dicts += j['output']\n",
    "        continue\n",
    "        for k in j['output']:\n",
    "            if 'feedback' in k:\n",
    "                print(k['feedback'])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df['ans_replaced'] == False]\n",
    "ans_rep = df[df['ans_replaced']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('closest_match').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_rep.groupby('closest_match').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14 / 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in qs:\n",
    "    if 'contact' in q['question']:\n",
    "        print(q['gtanswer'], q['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find ans_replaced \n",
    "- Look through previous input data\n",
    "- get videos that we already have uploaded\n",
    "- get examples from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk copy/easyturk/data/dataset/false_negatives52.txt', 'rb') as f:\n",
    "    false = json.load(f)\n",
    "    \n",
    "valid_false = []\n",
    "not_included = []\n",
    "for i in false:\n",
    "    if not i['ans_replaced']:\n",
    "        print(i)\n",
    "        \n",
    "    v_id = i['url'][-9:-4]\n",
    "    if v_id in v_ids:\n",
    "        valid_false.append(i)\n",
    "    else:\n",
    "        not_included.append(v_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_false = []\n",
    "not_included = []\n",
    "for i in false:\n",
    "    if not i['ans_replaced']:\n",
    "        print(i)\n",
    "        \n",
    "    v_id = i['url'][-9:-4]\n",
    "    if v_id in v_ids:\n",
    "        valid_false.append(i)\n",
    "    else:\n",
    "        not_included.append(v_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '../Charades_v1_480/%s.mp4'\n",
    "dest_path = '../AMT/easyturk/easyturk/videos/11-10/%s.mp4'\n",
    "\n",
    "for v_id in not_included:\n",
    "    src_file = src_path % v_id\n",
    "    dest_file = dest_path % v_id\n",
    "    shutil.copyfile(src_file, dest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old additions of false negatives\n",
    "\n",
    "- used valid_false\n",
    "- used not_included 0-15\n",
    "- used not_included 15-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_qs = qs[:45]\n",
    "\n",
    "initial_qs.insert(3, valid_false[0])\n",
    "initial_qs.insert(12, valid_false[1])\n",
    "initial_qs.insert(21, valid_false[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_45_falseneg.txt', 'w+') as f:\n",
    "    json.dump(initial_qs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_qs = qs[25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(next_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qs = next_qs + not_included[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_so_far = all_qs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, q in enumerate(all_qs):\n",
    "    if q['ans_replaced']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, q in enumerate(best_so_far):\n",
    "    if q['ans_replaced']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../AMT/easyturk/easyturk/data/dataset/input_155_falseneg.txt', 'w+') as f:\n",
    "    json.dump(best_so_far, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(by_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
