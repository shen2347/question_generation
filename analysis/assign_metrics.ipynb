{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../data/template_ids.txt') as json_file:\n",
    "    templ_ids_dict = json.load(json_file)\n",
    "    \n",
    "templ_ids = []\n",
    "\n",
    "for i in templ_ids_dict:\n",
    "    templ_ids.append(i)\n",
    "    \n",
    "#templ_ids.append('actWhatStart')\n",
    "    \n",
    "#templ_ids.append('actWhatEnd')\n",
    "\n",
    "\n",
    "banned_templates = ['actWhatBetweenWhileEndOneLast',\n",
    "                    'actWhatBetweenWhileStartOneLast',\n",
    "                    'actWhatBetweenWhileEndOneFirst',\n",
    "                    'actWhatBetweenWhileStartOneFirst',\n",
    "                    'actWhatStart',\n",
    "                    'actWhatEnd',\n",
    "                    'actCountChooseMore',\n",
    "                    'actCountChooseFewer',\n",
    "                    'actCount'\n",
    "                    'relFirst',\n",
    "                    'relLast',\n",
    "                    'actExists']\n",
    "\n",
    "\n",
    "for i in banned_templates:\n",
    "    templ_ids.append(i)\n",
    "len(templ_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('../../data/video_lengths.pkl', 'rb')\n",
    "LENGTHS = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "infile = open('../../data/stsgs/test_stsgs.pkl', 'rb')\n",
    "test_stsgs = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('../../data/stsgs/train_stsgs.pkl', 'rb')\n",
    "train_stsgs = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_path = '../../exports/dataset/balanced/train/'\n",
    "#train_stsgs = [f for f in listdir(train_path) if isfile(join(train_path, f))]\n",
    "#train_stsgs = [i[:5] for i in train_stsgs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/templ2global.txt', 'rb') as f:\n",
    "    templ2global = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_comp_title = 'illegal_comp'\n",
    "same_direct_title = 'same_direct'\n",
    "steps_title = 'steps'\n",
    "ans_type_title = 'ans_type'\n",
    "global_title = 'global_vars'\n",
    "lengths_title = 'lengths'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQ(q_id):\n",
    "    v_id = q_id.split(\"-\")[0]\n",
    "    return QA[v_id][q_id]\n",
    "\n",
    "def qaTotal():\n",
    "    total = 0\n",
    "    for v_id in QA:\n",
    "        total = total + len(QA[v_id])\n",
    "    print(\"QA Total: \", total)\n",
    "\n",
    "def printLens(dic, name):\n",
    "    print()\n",
    "    print(name)\n",
    "    total = 0\n",
    "    for i in dic:\n",
    "        print(i, len(dic[i]))\n",
    "        total = total + len(dic[i])\n",
    "    print(\"TOTAL: \", total)\n",
    "    \n",
    "def pickleDump(var, destination):\n",
    "    f = open(destination,\"wb\")\n",
    "    pickle.dump(var,f)\n",
    "    f.close() \n",
    "    \n",
    "def pickleLoad(path):\n",
    "    file = open(path, 'rb')\n",
    "    info = pickle.load(file)\n",
    "    file.close()\n",
    "    return info\n",
    "\n",
    "    \n",
    "def dumpMetrics(var, title, d_path, group, balanced):\n",
    "    \n",
    "    if type(balanced) == bool:\n",
    "        print(\"in dump metrics, balanced should be text\")\n",
    "        return\n",
    "    path = '../../exports/%s/metrics/%s/%s/%s.pkl' % (d_path, balanced, group, title)\n",
    "    print(\"dumping to \", path)\n",
    "    print()\n",
    "    pickleDump(var, path)\n",
    "    \n",
    "def loadMetrics(title, d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    path = '../../exports/%s/metrics/%s/%s/%s.pkl' % (d_path, balanced, group, title)\n",
    "    \n",
    "    return pickleLoad(path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVideoIDS(group):\n",
    "    if group == 'train':\n",
    "        return list(train_stsgs)\n",
    "    elif group == 'test':\n",
    "        return list(test_stsgs)\n",
    "    \n",
    "    elif group == 'names':\n",
    "        return names_stsgs\n",
    "    \n",
    "def loadJSON(path):\n",
    "    with open(path) as json_file:\n",
    "        QA = json.load(json_file)\n",
    "    return QA\n",
    "\n",
    "def getVideoQA(d_path, group, balanced, v_id):\n",
    "    if type(balanced) == bool: \n",
    "        print(\"balanced should not be bool\")\n",
    "    #if balanced == 'templ':\n",
    "    \n",
    "    if group == 'names':\n",
    "        group = 'test'\n",
    "    \n",
    "    QA = loadJSON('../../exports/%s/%s/%s/%s.txt' % (d_path, balanced, group, v_id))\n",
    "        \n",
    "   #elif balanced:\n",
    "   #     QA = loadJSON('../exports/%s/balanced/%s/%s.txt' % (d_path, group, v_id))\n",
    "# elif not balanced:\n",
    "   #     QA = loadJSON('../exports/%s/all/%s/%s.txt' % (d_path, group, v_id))\n",
    "        \n",
    "    return QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllOthers(QA, to_del):\n",
    "    \n",
    "    for q_id in to_del:\n",
    "        del QA[q_id]\n",
    "        \n",
    "    return list(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitQIDS(idx_dic, q_ids_dic):\n",
    "    split = {\n",
    "            'less': {},\n",
    "            'more': {}\n",
    "        }\n",
    "    for i in idx_dic:\n",
    "        split_idx = idx_dic[i]\n",
    "        for step in q_ids_dic[i]:\n",
    "            if step <= split_idx:\n",
    "                cat = 'less'\n",
    "            else:\n",
    "                cat = 'more'\n",
    "                \n",
    "            for q_id in q_ids_dic[i][step]:\n",
    "                v_id = q_id[:5]\n",
    "                if v_id not in split[cat]:\n",
    "                    split[cat][v_id] = []\n",
    "                split[cat][v_id].append(q_id)\n",
    "            \n",
    "    return split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YSKX3'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getVideoIDS('test')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel Compositions\n",
    "\n",
    "def getIllegalComp(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    ids = ['before', 'first', 'longer', 'repetition', 'objrel']\n",
    "\n",
    "    illegal_comp = {\n",
    "        'before': [],\n",
    "        'first': [],\n",
    "        'longer': [],\n",
    "        'repetition': [],\n",
    "        'objrel': []\n",
    "    }\n",
    "    v_ids = getVideoIDS(group)\n",
    "    test_id = 0\n",
    "    cnt = 0\n",
    "    comp_by_vid = {}\n",
    "    for v_id in v_ids:\n",
    "        comp_by_vid[v_id] = set()\n",
    "        if cnt == 0:\n",
    "            test_id = v_id\n",
    "        if cnt % 1000 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            metrics = q['metrics']\n",
    "            nov_comp = metrics['novel_comp']\n",
    "            for i in range(len(nov_comp)): \n",
    "                #if group == 'train':\n",
    "                if nov_comp[i]:\n",
    "                    illegal_comp[ids[i]].append(q_id)\n",
    "            #    if group == 'test':\n",
    "                    #if not nov_comp[i]:\n",
    "                        #illegal_comp[ids[i]].append(q_id)\n",
    "                    \n",
    "    for glob in illegal_comp:\n",
    "        print(glob)\n",
    "        for q_id in illegal_comp[glob]:\n",
    "            v_id = q_id[:5]\n",
    "            \n",
    "            #if group == 'train'\n",
    "            #if v_id not in comp_by_vid:\n",
    "            #    comp_by_vid[v_id] = set()\n",
    "            comp_by_vid[v_id].add(q_id)\n",
    "    \n",
    "    print(\"length of vidoe before: \", len(comp_by_vid[test_id]))\n",
    "    for i in comp_by_vid[test_id]:\n",
    "        print(i)\n",
    "\n",
    "    # right now its collected all the ones that have novel comps, but we want to retain those that don't in test set\n",
    "    if group == 'test':\n",
    "        for v_id in comp_by_vid:\n",
    "            QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "            \n",
    "\n",
    "            if v_id == test_id:\n",
    "                print(\"length of the whle qa\", len(QA))\n",
    "                \n",
    "            \n",
    "            comp_by_vid[v_id] = getAllOthers(QA, comp_by_vid[v_id])\n",
    "            \n",
    "            \n",
    "            for q_id in comp_by_vid[v_id]:\n",
    "                q = QA[q_id]\n",
    "                if q['metrics']['novel_comp'].count(True) > 0:\n",
    "                    print(\"There is a novel composition!!!\")\n",
    "\n",
    "    print(\"updated comb bby video\", len(comp_by_vid[test_id]))\n",
    "    \n",
    "    \n",
    "    return comp_by_vid, illegal_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect Ref Consistency\n",
    "\n",
    "# first, by templ/v_id and what asking\n",
    "\n",
    "def getIndirectRefConsistency(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    ids = ['object', 'relation', 'action', 'time']\n",
    "\n",
    "    same_direct = {}\n",
    "\n",
    "    cnt = 0\n",
    "    video_ids = getVideoIDS(group)\n",
    "    for v_id in video_ids:\n",
    "        if cnt % 100 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        same_direct[v_id] = {}\n",
    "        for i in templ_ids:\n",
    "            same_direct[v_id][i] = {}\n",
    "\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            t_id = q['attributes']['type']\n",
    "            indirects = q['metrics']['indirects']\n",
    "\n",
    "            direct = q['metrics']['direct_args']\n",
    "\n",
    "            sd_templ = same_direct[v_id][t_id]\n",
    "\n",
    "            if direct not in sd_templ:\n",
    "                sd_templ[direct] = {}\n",
    "                sd_templ[direct]['all'] = []\n",
    "                sd_templ[direct]['direct'] = []\n",
    "                sd_templ[direct]['object'] = []\n",
    "                sd_templ[direct]['relation'] = []\n",
    "                sd_templ[direct]['action'] = []\n",
    "                sd_templ[direct]['time'] = []\n",
    "                sd_templ[direct]['one'] = {}\n",
    "                sd_templ[direct]['one']['object'] = []\n",
    "                sd_templ[direct]['one']['relation'] = []\n",
    "                sd_templ[direct]['one']['action'] = []\n",
    "                sd_templ[direct]['one']['time'] = []\n",
    "                sd_templ[direct]['two'] = []\n",
    "\n",
    "                \n",
    "            #print('sddirect', sd_templ[direct])\n",
    "\n",
    "            sd_templ[direct]['all'].append(q_id)\n",
    "            \n",
    "            for idx, i in enumerate(indirects):\n",
    "                if i:\n",
    "                    #print(ids[idx])\n",
    "                    sd_templ[direct][ids[idx]].append(q_id)\n",
    "            \n",
    "\n",
    "            num_true = indirects.count(True)\n",
    "\n",
    "            if num_true == 0:\n",
    "                sd_templ[direct]['direct'].append(q_id)\n",
    "            elif num_true == 1:\n",
    "                chose_something = False\n",
    "                for i in range(len(indirects)):\n",
    "                    \n",
    "                    if indirects[i]:\n",
    "                        sd_templ[direct]['one'][ids[i]].append(q_id)\n",
    "                        chose_something = True\n",
    "                        #print()\n",
    "                        #print(q_id, ids[i])\n",
    "                        #print(sd_templ[direct]['one'][ids[i]])\n",
    "                #if not chose_something:\n",
    "                    #print(q_id, 'not chose something')\n",
    "            else:\n",
    "                sd_templ[direct]['two'].append(q_id)\n",
    "            \n",
    "        \n",
    "        #print()\n",
    "        #print(\"FINAL\")\n",
    "        #print('sddirect', sd_templ[direct]))\n",
    "        #print(same_direct[test_id]['andObjRelExists'])\n",
    "    return same_direct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of compositional steps\n",
    "def getNumSteps(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    steps = {}\n",
    "    steps_by_templ = {}\n",
    "    steps_by_glob = {}\n",
    "    \n",
    "    num_int = 20\n",
    "\n",
    "    for i in range(num_int):\n",
    "        steps[i] = []\n",
    "\n",
    "    v_ids = getVideoIDS(group)\n",
    "    count = 0\n",
    "    for v_id in v_ids:\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "        count = count + 1\n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            t = q['attributes']['type']\n",
    "            g = q['global']\n",
    "            #q = getQ(q_id)\n",
    "            q_steps = q['steps']\n",
    "\n",
    "            \n",
    "            if t not in steps_by_templ:\n",
    "                steps_by_templ[t] = {}\n",
    "                for i in range(num_int):\n",
    "                    steps_by_templ[t][i] = []\n",
    "                    \n",
    "            \n",
    "            if g not in steps_by_glob:\n",
    "                steps_by_glob[g] = {}\n",
    "                for i in range(num_int):\n",
    "                    steps_by_glob[g][i] = []\n",
    "                    \n",
    "                    \n",
    "            steps[q_steps].append(q_id)\n",
    "            steps_by_templ[t][q_steps].append(q_id)\n",
    "            steps_by_glob[g][q_steps].append(q_id)\n",
    "            \n",
    "    return steps, steps_by_templ, steps_by_glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Type\n",
    "# Global\n",
    "\n",
    "def getAnsTypeAndGlobal(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    answer_type = {\n",
    "        'binary': [],\n",
    "        'open': [],\n",
    "    }\n",
    "\n",
    "\n",
    "    old_glob = {\n",
    "        'exists': [],\n",
    "        'first': [],\n",
    "        'last': [],\n",
    "        'what': [],\n",
    "        'count': [],\n",
    "        'length': [],\n",
    "        'time': [],\n",
    "    }\n",
    "    \n",
    "    glob = {\n",
    "        'obj-rel': [],\n",
    "        'obj-act': [],\n",
    "        'rel-act': [],\n",
    "        'duration-comparison': [],\n",
    "        'superlative': [],\n",
    "        'exists': [],\n",
    "        'count': [],\n",
    "        'action-recognition': [],\n",
    "        'sequencing': [],\n",
    "    }\n",
    "    \n",
    "    answer_type = {\n",
    "        'binary': 0,\n",
    "        'open': 0,\n",
    "    }\n",
    "\n",
    "\n",
    "    old_glob = {\n",
    "        'exists': 0,\n",
    "        'first': 0,\n",
    "        'last': 0,\n",
    "        'what': 0,\n",
    "        'count': 0,\n",
    "        'length': 0,\n",
    "        'time': 0,\n",
    "    }\n",
    "    \n",
    "    glob = {\n",
    "        'obj-rel': 0,\n",
    "        'obj-act': 0,\n",
    "        'rel-act': 0,\n",
    "        'duration-comparison': 0,\n",
    "        'superlative': 0,\n",
    "        'exists': 0,\n",
    "        'count': 0,\n",
    "        'action-recognition': 0,\n",
    "        'sequencing': 0,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    templates = {}\n",
    "    \n",
    "    v_ids = getVideoIDS(group)\n",
    "    count = 0\n",
    "    for v_id in v_ids:\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "        count = count + 1\n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id] #getQ(q_id)\n",
    "            struct = q['attributes']['structural']\n",
    "            if struct == 'query':\n",
    "                q_ans_type = 'open'\n",
    "            else:\n",
    "                q_ans_type = 'binary'\n",
    "                \n",
    "            #answer_type[q_ans_type].append(q_id)\n",
    "            \n",
    "            \n",
    "            answer_type[q_ans_type] += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "            templ = q['attributes']['type']\n",
    "            for q_global in templ2global[templ]:\n",
    "                #glob[q_global].append(q_id)\n",
    "                glob[q_global] += 1\n",
    "            if templ not in templates:\n",
    "                #templates[templ] = []\n",
    "                templates[templ] = 0\n",
    "            #templates[templ].append(q_id)\n",
    "            templates[templ] += 1\n",
    "            \n",
    "            #q_ans_type = q['attributes']['ans_type']\n",
    "            #q_global = q['global']\n",
    "        \n",
    "    return answer_type, glob, templates\n",
    "\n",
    "#answer_type_a, global_a = getAnsTypeAndGlobal(QA_a)\n",
    "#answer_type_b, global_b = getAnsTypeAndGlobal(QA_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_lengths\n",
    "\n",
    "def getLengths(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    length_gen = {\n",
    "        'less': [],\n",
    "        'more': [],\n",
    "    }\n",
    "\n",
    "    threshold = 32\n",
    "\n",
    "    v_ids = getVideoIDS(group)\n",
    "    count = 0\n",
    "    for v_id in v_ids:\n",
    "        if count % 1000 == 0:\n",
    "            print(count)\n",
    "        count = count + 1\n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        length = LENGTHS[v_id]\n",
    "        q_ids = list(QA.keys())\n",
    "        new_ids = []\n",
    "        if length <= threshold:\n",
    "            length_gen['less'].append(v_id)\n",
    "        else:\n",
    "            length_gen['more'].append(v_id)\n",
    "\n",
    "    \n",
    "    return length_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, split it into steps dics by global and by template\n",
    "\n",
    "def addSteps(dic, cat, step):\n",
    "    if cat not in dic:\n",
    "        dic[cat] = {}\n",
    "    if step not in dic[cat]:\n",
    "        dic[cat][step] = 0\n",
    "    dic[cat][step] += 1\n",
    "    \n",
    "    return dic\n",
    "\n",
    "def splitCompoStepsByGlobTempl(d_path, group, balanced):\n",
    "    if type(balanced) == bool: \n",
    "        print(\"invalid balanced\")\n",
    "        return\n",
    "    by_t = {}\n",
    "    by_g = {}\n",
    "    \n",
    "    v_ids = list(getVideoIDS(group))\n",
    "    \n",
    "    for v_id in v_ids:\n",
    "        QA = getVideoQA(d_path, group, balanced, v_id) \n",
    "        \n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            \n",
    "            t = q['attributes']['type']\n",
    "            g = q['global']\n",
    "            s = q['steps']\n",
    "            \n",
    "            by_g = addSteps(by_g, g, s)\n",
    "            by_t = addSteps(by_t, t, s)\n",
    "            \n",
    "    return by_g, by_t\n",
    "       \n",
    "def splitSteps(dic, split_idx):\n",
    "    split = {\n",
    "        'less': 0,\n",
    "        'more': 0,\n",
    "    }\n",
    "        \n",
    "    for step in dic:\n",
    "        if step <= split_idx:\n",
    "            cat = 'less'\n",
    "        else:\n",
    "            cat = 'more'\n",
    "        split[cat] += dic[step]\n",
    "        \n",
    "    return split\n",
    "        \n",
    "def findUniversalSplit(dic, split_idx):\n",
    "    split = {}\n",
    "    for i in dic:\n",
    "        split[i] = splitSteps(dic[i], split_idx)\n",
    "    return split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findSplit(dic, perc):\n",
    "    tot = 0\n",
    "    copy_dic = {}\n",
    "    for i in dic:\n",
    "        tot += dic[i]\n",
    "        copy_dic[i] = dic[i]\n",
    "        \n",
    "    \n",
    "    split_point = int(tot * perc)\n",
    "    split_idx = 0\n",
    "    tot = 0\n",
    "    while tot < split_point:\n",
    "        min_key = min(copy_dic)\n",
    "        tot += copy_dic[min_key]\n",
    "        del copy_dic[min_key]\n",
    "        split_idx = min_key\n",
    "        \n",
    "    return split_idx\n",
    "\n",
    "def findGlobalSplit(dic, dic_templ, perc):\n",
    "    # first, find what the split should be, based on the perc\n",
    "    s = {}\n",
    "    s_templ = {}\n",
    "    s_cnt = {}\n",
    "    for i in dic:\n",
    "        s_idx = findSplit(dic[i], perc)\n",
    "        \n",
    "        s[i] = splitSteps(dic[i], s_idx)\n",
    "        s_cnt[i] = s_idx\n",
    "        \n",
    "    if dic_templ != None:\n",
    "        for t in dic_templ:\n",
    "            glob = t_to_g[t]\n",
    "            idx = s_cnt[glob]\n",
    "            s_templ[t] = splitSteps(dic_templ[t], idx)\n",
    "        \n",
    "    return s_cnt, s, s_templ\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "illegal_comp_title = 'illegal_comp'\n",
    "illegal_comp_by_type_title = 'illegal_comp_by_typ'\n",
    "same_direct_title = 'same_direct'\n",
    "steps_title = 'steps'\n",
    "steps_by_templ_title = 'steps_by_templ'\n",
    "ans_type_title = 'ans_type'\n",
    "global_title = 'global_vars'\n",
    "lengths_title = 'lengths'\n",
    "templ_title = 'templates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate ans/glob/templ\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "dumping to  ../../exports/dataset/metrics/balanced/train/ans_type.pkl\n",
      "\n",
      "dumping to  ../../exports/dataset/metrics/balanced/train/global_vars.pkl\n",
      "\n",
      "dumping to  ../../exports/dataset/metrics/balanced/train/templates.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'balanced'\n",
    "group = 'train'\n",
    "save = True\n",
    "\n",
    "il_comp = False\n",
    "step = False\n",
    "AGT = True\n",
    "lens = False\n",
    "indir = False\n",
    "\n",
    "if il_comp: # CHECKED\n",
    "    print(\"Generate Illegal comp\")\n",
    "    illegal_comp_by_vid_to_del, illegal_comp_by_type = getIllegalComp(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(illegal_comp_by_vid_to_del, illegal_comp_title, d_path, group, balanced)\n",
    "        dumpMetrics(illegal_comp_by_type, illegal_comp_by_type_title, d_path, group, balanced)\n",
    "\n",
    "if step: # CHECKED splitting by templ  \n",
    "    print(\"Generate Steps\")\n",
    "    print(\"Split comp by glob\")\n",
    "    g, t = splitCompoStepsByGlobTempl(d_path,  group, balanced)\n",
    "    print(\"Find global split\")\n",
    "    sp_t_idx, sp_t, _ = findGlobalSplit(t, None, .5)\n",
    "    print(\"Find number of steps\")\n",
    "    steps_b, steps_b_templ, steps_b_glob = getNumSteps(d_path, group, balanced)\n",
    "    print(\"split Q ids\")\n",
    "    steps_templ_sp = splitQIDS(sp_t_idx, steps_b_templ)\n",
    "    if save:\n",
    "        dumpMetrics(steps_templ_sp, steps_by_templ_title, d_path, group, balanced)\n",
    "        #dumpMetrics(steps_tr_b, steps_title, d_path, group, balanced)\n",
    "    \n",
    "if AGT: # CHECKED\n",
    "    print(\"Generate ans/glob/templ\")    \n",
    "    ans_type_tr_b, global_tr_b, templ_tr_b = getAnsTypeAndGlobal(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(ans_type_tr_b, ans_type_title, d_path, group, balanced)\n",
    "        dumpMetrics(global_tr_b, global_title, d_path, group, balanced)\n",
    "        dumpMetrics(templ_tr_b, templ_title, d_path, group, balanced)\n",
    "    \n",
    "if lens: # CHECKED\n",
    "    print(\"Generate Lengths\")\n",
    "    lengths_tr_b = getLengths(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(lengths_tr_b, lengths_title, d_path, group, balanced)\n",
    "\n",
    "if indir:\n",
    "    print(\"Generate Indirect ref consistency\")\n",
    "    same_direct_tr_b = getIndirectRefConsistency(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(same_direct_tr_b, same_direct_title, d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate ans/glob/templ\n",
      "0\n",
      "1000\n",
      "dumping to  ../../exports/dataset/metrics/balanced/test/ans_type.pkl\n",
      "\n",
      "dumping to  ../../exports/dataset/metrics/balanced/test/global_vars.pkl\n",
      "\n",
      "dumping to  ../../exports/dataset/metrics/balanced/test/templates.pkl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'balanced'\n",
    "group = 'test'\n",
    "save = True\n",
    "\n",
    "il_comp = False\n",
    "step = False\n",
    "AGT = True\n",
    "lens = False\n",
    "indir = False\n",
    "\n",
    "if il_comp: # CHECKED\n",
    "    print(\"Generate Illegal comp\")\n",
    "    illegal_comp_by_vid_to_del, illegal_comp_by_type = getIllegalComp(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(illegal_comp_by_vid_to_del, illegal_comp_title, d_path, group, balanced)\n",
    "        dumpMetrics(illegal_comp_by_type, illegal_comp_by_type_title, d_path, group, balanced)\n",
    "\n",
    "if step: # CHECKED splitting by templ  \n",
    "    print(\"Generate Steps\")\n",
    "    print(\"Split comp by glob\")\n",
    "    g, t = splitCompoStepsByGlobTempl(d_path,  group, balanced)\n",
    "    print(\"Find global split\")\n",
    "    sp_t_idx, sp_t, _ = findGlobalSplit(t, None, .5)\n",
    "    print(\"Find number of steps\")\n",
    "    steps_b, steps_b_templ, steps_b_glob = getNumSteps(d_path, group, balanced)\n",
    "    print(\"split Q ids\")\n",
    "    steps_templ_sp = splitQIDS(sp_t_idx, steps_b_templ)\n",
    "    if save:\n",
    "        dumpMetrics(steps_templ_sp, steps_by_templ_title, d_path, group, balanced)\n",
    "        #dumpMetrics(steps_tr_b, steps_title, d_path, group, balanced)\n",
    "    \n",
    "if AGT: # CHECKED\n",
    "    print(\"Generate ans/glob/templ\")    \n",
    "    ans_type_tr_b, global_tr_b, templ_tr_b = getAnsTypeAndGlobal(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(ans_type_tr_b, ans_type_title, d_path, group, balanced)\n",
    "        dumpMetrics(global_tr_b, global_title, d_path, group, balanced)\n",
    "        dumpMetrics(templ_tr_b, templ_title, d_path, group, balanced)\n",
    "    \n",
    "if lens: # CHECKED\n",
    "    print(\"Generate Lengths\")\n",
    "    lengths_tr_b = getLengths(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(lengths_tr_b, lengths_title, d_path, group, balanced)\n",
    "\n",
    "if indir:\n",
    "    print(\"Generate Indirect ref consistency\")\n",
    "    same_direct_tr_b = getIndirectRefConsistency(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(same_direct_tr_b, same_direct_title, d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate Illegal comp\n",
      "0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../exports/dataset/all/train/46GP8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e9cc57de3459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mil_comp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# CHECKED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generate Illegal comp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0millegal_comp_by_vid_to_del\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0millegal_comp_by_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetIllegalComp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdumpMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0millegal_comp_by_vid_to_del\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0millegal_comp_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-26d744e8f9db>\u001b[0m in \u001b[0;36mgetIllegalComp\u001b[0;34m(d_path, group, balanced)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mQA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVideoQA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b5cb05f76dbe>\u001b[0m in \u001b[0;36mgetVideoQA\u001b[0;34m(d_path, group, balanced, v_id)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mQA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../exports/%s/%s/%s/%s.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbalanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m    \u001b[0;31m#elif balanced:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b5cb05f76dbe>\u001b[0m in \u001b[0;36mloadJSON\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mQA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../exports/dataset/all/train/46GP8.txt'"
     ]
    }
   ],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'all'\n",
    "group = 'train'\n",
    "save = True\n",
    "\n",
    "il_comp = True\n",
    "step = True\n",
    "AGT = False\n",
    "lens = False\n",
    "indir = False\n",
    "\n",
    "if il_comp: # CHECKED\n",
    "    print(\"Generate Illegal comp\")\n",
    "    illegal_comp_by_vid_to_del, illegal_comp_by_type = getIllegalComp(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(illegal_comp_by_vid_to_del, illegal_comp_title, d_path, group, balanced)\n",
    "        dumpMetrics(illegal_comp_by_type, illegal_comp_by_type_title, d_path, group, balanced)\n",
    "\n",
    "if step: # CHECKED splitting by templ  \n",
    "    print(\"Generate Steps\")\n",
    "    print(\"Split comp by glob\")\n",
    "    g, t = splitCompoStepsByGlobTempl(d_path,  group, balanced)\n",
    "    print(\"Find global split\")\n",
    "    sp_t_idx, sp_t, _ = findGlobalSplit(t, None, .5)\n",
    "    print(\"Find number of steps\")\n",
    "    steps_b, steps_b_templ, steps_b_glob = getNumSteps(d_path, group, balanced)\n",
    "    print(\"split Q ids\")\n",
    "    steps_templ_sp = splitQIDS(sp_t_idx, steps_b_templ)\n",
    "    if save:\n",
    "        dumpMetrics(steps_templ_sp, steps_by_templ_title, d_path, group, balanced)\n",
    "        #dumpMetrics(steps_tr_b, steps_title, d_path, group, balanced)\n",
    "    \n",
    "if AGT: # CHECKED\n",
    "    print(\"Generate ans/glob/templ\")    \n",
    "    ans_type_tr_b, global_tr_b, templ_tr_b = getAnsTypeAndGlobal(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(ans_type_tr_b, ans_type_title, d_path, group, balanced)\n",
    "        dumpMetrics(global_tr_b, global_title, d_path, group, balanced)\n",
    "        dumpMetrics(templ_tr_b, templ_title, d_path, group, balanced)\n",
    "    \n",
    "if lens: # CHECKED\n",
    "    print(\"Generate Lengths\")\n",
    "    lengths_tr_b = getLengths(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(lengths_tr_b, lengths_title, d_path, group, balanced)\n",
    "\n",
    "if indir:\n",
    "    print(\"Generate Indirect ref consistency\")\n",
    "    same_direct_tr_b = getIndirectRefConsistency(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(same_direct_tr_b, same_direct_title, d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'all'\n",
    "group = 'test'\n",
    "save = True\n",
    "\n",
    "il_comp = False\n",
    "step = False\n",
    "AGT = False\n",
    "lens = False\n",
    "indir = True\n",
    "\n",
    "if il_comp: # CHECKED\n",
    "    print(\"Generate Illegal comp\")\n",
    "    illegal_comp_by_vid_to_del, illegal_comp_by_type = getIllegalComp(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(illegal_comp_by_vid_to_del, illegal_comp_title, d_path, group, balanced)\n",
    "        dumpMetrics(illegal_comp_by_type, illegal_comp_by_type_title, d_path, group, balanced)\n",
    "\n",
    "if step: # CHECKED splitting by templ  \n",
    "    print(\"Generate Steps\")\n",
    "    print(\"Split comp by glob\")\n",
    "    g, t = splitCompoStepsByGlobTempl(d_path,  group, balanced)\n",
    "    print(\"Find global split\")\n",
    "    sp_t_idx, sp_t, _ = findGlobalSplit(t, None, .5)\n",
    "    print(\"Find number of steps\")\n",
    "    steps_b, steps_b_templ, steps_b_glob = getNumSteps(d_path, group, balanced)\n",
    "    print(\"split Q ids\")\n",
    "    steps_templ_sp = splitQIDS(sp_t_idx, steps_b_templ)\n",
    "    if save:\n",
    "        dumpMetrics(steps_templ_sp, steps_by_templ_title, d_path, group, balanced)\n",
    "        #dumpMetrics(steps_tr_b, steps_title, d_path, group, balanced)\n",
    "    \n",
    "if AGT: # CHECKED\n",
    "    print(\"Generate ans/glob/templ\")    \n",
    "    ans_type_tr_b, global_tr_b, templ_tr_b = getAnsTypeAndGlobal(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(ans_type_tr_b, ans_type_title, d_path, group, balanced)\n",
    "        dumpMetrics(global_tr_b, global_title, d_path, group, balanced)\n",
    "        dumpMetrics(templ_tr_b, templ_title, d_path, group, balanced)\n",
    "    \n",
    "if lens: # CHECKED\n",
    "    print(\"Generate Lengths\")\n",
    "    lengths_tr_b = getLengths(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(lengths_tr_b, lengths_title, d_path, group, balanced)\n",
    "\n",
    "if indir:\n",
    "    print(\"Generate Indirect ref consistency\")\n",
    "    same_direct_tr_b = getIndirectRefConsistency(d_path, group, balanced)\n",
    "    if save:\n",
    "        dumpMetrics(same_direct_tr_b, same_direct_title, d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumpMetrics(templ_tr_b, templ_title, d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does temporal Change answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indirect Ref Consistency\n",
    "\n",
    "# first, by templ/v_id and what asking\n",
    "\n",
    "def getIndirectRefConsistencyTime(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    ids = ['object', 'relation', 'action', 'time']\n",
    "\n",
    "    same_direct = {}\n",
    "    comp = {}\n",
    "\n",
    "    cnt = 0\n",
    "    video_ids = getVideoIDS(group)\n",
    "    for v_id in video_ids:\n",
    "        if cnt % 100 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        comp[v_id] = {}\n",
    "        \n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        same_direct[v_id] = {}\n",
    "        for i in templ_ids:\n",
    "            same_direct[v_id][i] = {}\n",
    "\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            t_id = q['attributes']['type']\n",
    "            indirects = q['metrics']['indirects']\n",
    "\n",
    "            direct = q['metrics']['direct_args']\n",
    "\n",
    "            sd_templ = same_direct[v_id][t_id]\n",
    "\n",
    "            if direct not in sd_templ:\n",
    "                sd_templ[direct] = {}\n",
    "                sd_templ[direct]['all'] = []\n",
    "                sd_templ[direct]['no_time'] = []\n",
    "                sd_templ[direct]['time'] = []\n",
    "\n",
    "                \n",
    "\n",
    "            sd_templ[direct]['all'].append(q_id)\n",
    "            \n",
    "            if indirects[3]:\n",
    "                sd_templ[direct]['time'].append(q_id)\n",
    "            else:\n",
    "                sd_templ[direct]['no_time'].append(q_id)\n",
    "\n",
    "        \n",
    "        ### think this may have only done stuff for the last sd_templ?\n",
    "        #for direct in sd_templ:\n",
    "        #    thing = sd_templ[direct]\n",
    "        \n",
    "        for t_id in same_direct[v_id]:\n",
    "            for direct in same_direct[v_id][t_id]:\n",
    "                thing = same_direct[v_id][t_id][direct]\n",
    "                no_time = thing['no_time']\n",
    "\n",
    "                if len(no_time) > 0:\n",
    "                    no_time_ans = QA[no_time[0]]['answer']\n",
    "                else:\n",
    "                    no_time_ans = \"None without time\"\n",
    "\n",
    "                time = thing['time']\n",
    "                for q_id in time:\n",
    "                    q_ans = QA[q_id]['answer']\n",
    "                    if no_time_ans == \"None without time\":\n",
    "                        #no_direct.append(q_id)\n",
    "                        comp[v_id][q_id] = 'no_direct'\n",
    "                    elif no_time_ans == q_ans:\n",
    "                        #same_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'same_ans'\n",
    "                    else:\n",
    "                        #diff_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'diff_ans'\n",
    "\n",
    "                for q_id in thing['no_time']:\n",
    "                    q_ans = QA[q_id]['answer']\n",
    "                    comp[v_id][q_id] = 'no_temporal'\n",
    "                    #num_true = indirects.count(True)\n",
    "\n",
    "                #if num_true == 0:\n",
    "                #    sd_templ[direct]['direct'].append(q_id)\n",
    "                #elif num_true == 1:\n",
    "                #    chose_something = False\n",
    "                #    for i in range(len(indirects)):\n",
    "\n",
    "                #        if indirects[i]:\n",
    "                #            sd_templ[direct]['one'][ids[i]].append(q_id)\n",
    "                #            chose_something = True\n",
    "                            #print()\n",
    "                            #print(q_id, ids[i])\n",
    "                            #print(sd_templ[direct]['one'][ids[i]])\n",
    "                    #if not chose_something:\n",
    "                        #print(q_id, 'not chose something')\n",
    "                #else:\n",
    "                #    sd_templ[direct]['two'].append(q_id)\n",
    "    print(\"Saving temporal change ans in ../exports/%s/metrics/%s/%s/temporalChangeAns.txt\" % (d_path, balanced, group))\n",
    "    with open('../exports/%s/metrics/%s/%s/temporalChangeAns.txt' % (d_path, balanced, group), 'w+') as f:\n",
    "        json.dump(comp, f)\n",
    "    f.close()\n",
    "    \n",
    "        \n",
    "        \n",
    "    return same_direct, comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably don't need mult now\n",
    "\n",
    "\n",
    "def doesTempLocChangeAnswer(d_path, balanced, group, same_direct): \n",
    "    #diff_pairs = []\n",
    "    #same_pairs = []\n",
    "    #no_direct = []\n",
    "    \n",
    "    #if adding:\n",
    "    if same_direct is None:\n",
    "        with open('../exports/%s/metrics/%s/%s/temporalChangeAns.txt' % (d_path, balanced, group), 'rb') as f:\n",
    "            same_direct = json.load(f)\n",
    "        f.close()\n",
    "    #else:\n",
    "    comp = {}\n",
    "    \n",
    "    \n",
    "    for i, v_id in enumerate(same_direct): \n",
    "        if i % 2 == 0:\n",
    "            print(i)\n",
    "        #with open('../exports/%s/%s/%s/%s.txt' % (d_path, balanced, group, v_id)) as json_file:\n",
    "       #         QA = json.load(json_file)\n",
    "\n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        #comp[v_id] = {}\n",
    "        #if adding:\n",
    "        #    video_mapping = templ_change_ans[v_id]\n",
    "        #    cnt = len(video_mapping)\n",
    "        \n",
    "        if v_id not in comp: \n",
    "            comp[v_id] = {}\n",
    "            \n",
    "        for t_id in same_direct[v_id]:\n",
    "            #print(\" \"*4, t_id)\n",
    "            for local in same_direct[v_id][t_id]:\n",
    "                print(same_direct[v_id][t_id])\n",
    "                thing = same_direct[v_id][t_id][local]\n",
    "                \n",
    "                no_time = thing['no_time']\n",
    "\n",
    "                if len(no_time) > 0:\n",
    "                    no_time_ans = QA[no_time[0]]['answer']\n",
    "                else:\n",
    "                    no_time_ans = \"None without time\"\n",
    "                    \n",
    "                    \n",
    "                time = thing['time']\n",
    "\n",
    "                for q_id in time:\n",
    "                    #print(\" \"*12, QA[q_id]['answer'], QA[q_id]['question'])\n",
    "                    #if q_id in \n",
    "                    q_ans = QA[q_id]['answer']\n",
    "                    if no_time_ans == \"None without time\":\n",
    "                        #no_direct.append(q_id)\n",
    "                        comp[v_id][q_id] = 'no_direct'\n",
    "                    elif no_time_ans == q_ans:\n",
    "                        #same_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'same_ans'\n",
    "                    else:\n",
    "                        #diff_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'diff_ans'\n",
    "                        \n",
    "                for q_id in thing['no_time']:\n",
    "                    q_ans = QA[q_id]['answer']\n",
    "                    comp[v_id][q_id] = 'no_temporal'\n",
    "\n",
    "        #if cnt % 1000 == 0:\n",
    "        #    print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    #with open('../exports/dataset/metrics/%s/%s/diff_pairs_one.txt' % (balanced, group), 'w+') as f:\n",
    "    #    json.dump(diff_pairs, f)\n",
    "    #f.close()\n",
    "    #with open('../exports/dataset/metrics/%s/%s/same_pairs_one.txt' % (balanced, group), 'w+') as f:\n",
    "    #    json.dump(same_pairs, f)\n",
    "    #f.close()\n",
    "    #with open('../exports/dataset/metrics/%s/%s/no_direct_one.txt' % (balanced, group), 'w+') as f:\n",
    "    #    json.dump(no_direct, f)\n",
    "    #f.close()\n",
    "    print(\"Saving temporal change ans in ../exports/%s/metrics/%s/%s/temporalChangeAns.txt\" % (d_path, balanced, group))\n",
    "    with open('../exports/%s/metrics/%s/%s/temporalChangeAns.txt' % (d_path, balanced, group), 'w+') as f:\n",
    "        json.dump(comp, f)\n",
    "    f.close()\n",
    "    \n",
    "        \n",
    "    return comp\n",
    "\n",
    "\n",
    "#same_direct_te = getIndirectRefConsistencyTime(d_path, 'test', 'all')\n",
    "#comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"test\", same_direct_te)\n",
    "#diff_pairs_train, same_pairs_train, no_direct_train, comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templ_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'train'\n",
    "balanced = 'all'\n",
    "same_direct_tr = getIndirectRefConsistencyTime(d_path, group, balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../exports/dataset/metrics/all/train/temporalChangeAns.txt', 'rb') as f:\n",
    "    same_direct_train = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set()\n",
    "for i in same_direct_train:\n",
    "    #print(i, same_direct_train[i])\n",
    "    for j in same_direct_train[i]:\n",
    "        x.add(same_direct_train[i][j])\n",
    "    break\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_train = doesTempLocChangeAnswer(d_path, \"all\", \"train\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_direct_te = getIndirectRefConsistencyTime(d_path, 'test', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novel Compositions\n",
    "\n",
    "def getIllegalCompOLD(d_path, group, balanced):\n",
    "    if type(balanced) == bool:\n",
    "        print(\"balanced should be text\")\n",
    "        return\n",
    "    ids = ['before', 'first', 'longer', 'repetition', 'objrel']\n",
    "\n",
    "    illegal_comp = {\n",
    "        'before': [],\n",
    "        'first': [],\n",
    "        'longer': [],\n",
    "        'repetition': [],\n",
    "        'objrel': []\n",
    "    }\n",
    "    v_ids = getVideoIDS(group)\n",
    "    test_id = 0\n",
    "    cnt = 0\n",
    "    for v_id in v_ids:\n",
    "        if cnt == 0:\n",
    "            test_id = v_id\n",
    "        if cnt % 1000 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "        QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "        for q_id in QA:\n",
    "            q = QA[q_id]\n",
    "            metrics = q['metrics']\n",
    "            nov_comp = metrics['novel_comp']\n",
    "            for i in range(len(nov_comp)): \n",
    "                #if group == 'train':\n",
    "                if nov_comp[i]:\n",
    "                    illegal_comp[ids[i]].append(q_id)\n",
    "            #    if group == 'test':\n",
    "                    #if not nov_comp[i]:\n",
    "                        #illegal_comp[ids[i]].append(q_id)\n",
    "    \n",
    "    comp_by_vid = {}  \n",
    "    for glob in illegal_comp:\n",
    "        print(glob)\n",
    "        for q_id in illegal_comp[glob]:\n",
    "            v_id = q_id[:5]\n",
    "            \n",
    "            if group == 'train'\n",
    "            if v_id not in comp_by_vid:\n",
    "                comp_by_vid[v_id] = set()\n",
    "            comp_by_vid[v_id].add(q_id)\n",
    "    \n",
    "    print(\"length of vidoe before: \", len(comp_by_vid[test_id]))\n",
    "    for i in comp_by_vid[test_id]:\n",
    "        print(i)\n",
    "\n",
    "    # right now its collected all the ones that have novel comps, but we want to retain those that don't in test set\n",
    "    if group == 'test':\n",
    "        for v_id in comp_by_vid:\n",
    "            QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "\n",
    "            if v_id == test_id:\n",
    "                print(\"length of the whle qa\", len(QA))\n",
    "                \n",
    "            \n",
    "            comp_by_vid[v_id] = getAllOthers(QA, comp_by_vid[v_id])\n",
    "\n",
    "    print(\"updated comb bby video\", len(comp_by_vid[test_id]))\n",
    "    \n",
    "    \n",
    "    return comp_by_vid, illegal_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in same_direct_te:\n",
    "    print(i, same_direct_te[i])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"test\", None)# same_direct_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_direct_te = getIndirectRefConsistencyTime(d_path, 'test', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"names\", None)# same_direct_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'all'\n",
    "group = 'test'\n",
    "with open('../exports/%s/metrics/%s/%s/temporalChangeAns.txt' % (d_path, balanced, group), 'rb') as f:\n",
    "    same_direct = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(same_direct)[:10]:\n",
    "    QA = getVideoQA(d_path, group, balanced, i)\n",
    "    QA_b = getVideoQA('dataset CVPR submission', 'test', 'balanced', i)\n",
    "    print(len(same_direct[i]), len(QA_b), len(QA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in same_direct:\n",
    "    print(i)\n",
    "    for j in same_direct[i]:\n",
    "        print(\"    \", j)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by template or by global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_to_g = {'objExists': 'what',\n",
    " 'relExists': 'exists',\n",
    " 'actExists': 'exists',\n",
    " 'objWhat': 'what',\n",
    " 'objFirst': 'first',\n",
    " 'objFirstVerify': 'first',\n",
    " 'objLast': 'last',\n",
    " 'actLengthLongerChoose': 'length',\n",
    " 'actLengthShorterChoose': 'length',\n",
    " 'relTime': 'time',\n",
    " 'objTime': 'time',\n",
    " 'objWhatGeneral': 'what',\n",
    " 'actCountChooseMore': 'count',\n",
    " 'actCountChooseFewer': 'count',\n",
    " 'objRelExists': 'exists',\n",
    " 'andObjRelExists': 'exists',\n",
    " 'xorObjRelExists': 'exists',\n",
    " 'objWhatChoose': 'what',\n",
    " 'objFirstChoose': 'first',\n",
    " 'objLastChoose': 'last',\n",
    " 'objLastVerify': 'last',\n",
    " 'actCount': 'count',\n",
    " 'actLengthLongerVerify': 'length',\n",
    " 'actLengthShorterVerify': 'length',\n",
    " 'actTime': 'time',\n",
    " 'relLast': 'last',\n",
    " 'relFirst': 'first'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Pairs with same answer after localize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doesTempLocChangeAnswerOLD(d_path, balanced, group, same_direct): \n",
    "    diff_pairs = []\n",
    "    same_pairs = []\n",
    "    no_direct = []\n",
    "    \n",
    "    comp = {}\n",
    "\n",
    "    cnt = 0\n",
    "    for i, v_id in enumerate(same_direct): \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        if group == 'names'\n",
    "        with open('../exports/%s/%s/%s/%s.txt' % (d_path, balanced, group, v_id)) as json_file:\n",
    "                QA = json.load(json_file)\n",
    "\n",
    "        comp[v_id] = {}\n",
    "        for t_id in same_direct[v_id]:\n",
    "            #print(\" \"*4, t_id)\n",
    "            for direct in same_direct[v_id][t_id]:\n",
    "                thing = same_direct[v_id][t_id][direct]\n",
    "                direct = thing['direct']\n",
    "\n",
    "                if len(direct) > 0:\n",
    "                    #print(\" \"*8, QA[direct[0]]['answer'], QA[direct[0]]['question'])\n",
    "                    direct_ans = QA[direct[0]]['answer']\n",
    "                else:\n",
    "                    direct_ans = \"No Direct\"\n",
    "                time = thing['one']['time']\n",
    "\n",
    "                for q_id in time:\n",
    "                    #print(\" \"*12, QA[q_id]['answer'], QA[q_id]['question'])\n",
    "                    q_ans = QA[q_id]['answer']\n",
    "                    if direct_ans == \"No Direct\":\n",
    "                        no_direct.append(q_id)\n",
    "                        comp[v_id][q_id] = 'no_direct'\n",
    "                    elif direct_ans == q_ans:\n",
    "                        same_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'same_ans'\n",
    "                    else:\n",
    "                        diff_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'diff_ans'\n",
    "\n",
    "        if cnt % 1000 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    with open('../exports/dataset/metrics/%s/%s/diff_pairs_one.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(diff_pairs, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/same_pairs_one.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(same_pairs, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/no_direct_one.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(no_direct, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/comp_one.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(comp, f)\n",
    "    f.close()\n",
    "    \n",
    "        \n",
    "    return diff_pairs, same_pairs, no_direct, comp\n",
    "\n",
    "#d_path = 'dataset before 11-4'\n",
    "\n",
    "#same_direct_te = getIndirectRefConsistencyTime(d_path, 'test', 'all')\n",
    "#diff_pairs_test, same_pairs_test, no_direct_test, comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"test\", same_direct_te)\n",
    "#diff_pairs_train, same_pairs_train, no_direct_train, comp_test = doesTempLocChangeAnswer(d_path, \"all\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing when have multiple. \n",
    "\n",
    "def doesTempLocChangeAnswerMultIndirectOLD(d_path, balanced, group, same_direct): \n",
    "    diff_pairs = []\n",
    "    same_pairs = []\n",
    "    no_direct = []\n",
    "\n",
    "    comp = {}\n",
    "    cnt = 0\n",
    "    for v_id in same_direct: \n",
    "        with open('../exports/%s/%s/%s/%s.txt' % (d_path, balanced, group, v_id)) as json_file:\n",
    "                QA = json.load(json_file)\n",
    "\n",
    "        comp[v_id] = {}\n",
    "        for t_id in same_direct[v_id]:\n",
    "            #print(\" \"*4, t_id)\n",
    "            for direct in same_direct[v_id][t_id]:\n",
    "                thing = same_direct[v_id][t_id][direct]\n",
    "                direct = thing['direct']\n",
    "\n",
    "                if len(direct) > 0:\n",
    "                    #print(\" \"*8, QA[direct[0]]['answer'], QA[direct[0]]['question'])\n",
    "                    direct_ans = QA[direct[0]]['answer']\n",
    "                else:\n",
    "                    direct_ans = \"No Direct\"\n",
    "                mult = thing['two']\n",
    "                #time = thing['one']['time']\n",
    "\n",
    "                for q_id in mult:\n",
    "                    #print(\" \"*12, QA[q_id]['answer'], QA[q_id]['question'])\n",
    "                    q = QA[q_id]\n",
    "                    \n",
    "                    indirects = q['metrics']['indirects']\n",
    "                    if not indirects[3]:\n",
    "                        continue\n",
    "                        \n",
    "                    q_ans = q['answer']\n",
    "                    if direct_ans == \"No Direct\":\n",
    "                        no_direct.append(q_id)\n",
    "                        comp[v_id][q_id] = 'no_direct'\n",
    "                    elif direct_ans == q_ans:\n",
    "                        same_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'same_ans'\n",
    "                    else:\n",
    "                        diff_pairs.append((direct[0], q_id))\n",
    "                        comp[v_id][q_id] = 'diff_ans'\n",
    "\n",
    "        if cnt % 1000 == 0:\n",
    "            print(cnt)\n",
    "        cnt = cnt + 1\n",
    "        \n",
    "    \n",
    "    with open('../exports/dataset/metrics/%s/%s/diff_pairs_mult.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(diff_pairs, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/same_pairs_mult.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(same_pairs, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/no_direct_mult.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(no_direct, f)\n",
    "    f.close()\n",
    "    with open('../exports/dataset/metrics/%s/%s/comp_mult.txt' % (balanced, group), 'w+') as f:\n",
    "        json.dump(comp, f)\n",
    "    f.close()\n",
    "    return diff_pairs, same_pairs, no_direct, comp\n",
    "\n",
    "\n",
    "#diff_pairs_test_mult, same_pairs_test_mult, no_direct_test_mult, comp_test_mult = doesTempLocChangeAnswerMultIndirect(d_path, \"all\", \"test\", same_direct_te)\n",
    "#diff_pairs_train_mult, same_pairs_train_mult, no_direct_train_mult, comp_train_mult = doesTempLocChangeAnswerMultIndirect(\"all\", \"train\", same_direct_tr_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_temp = 0\n",
    "same = 0\n",
    "diff = 0\n",
    "no_dir = 0\n",
    "\n",
    "for v_id in comp_test:\n",
    "    for q_id in comp_test[v_id]:\n",
    "        val = comp_test[v_id][q_id]\n",
    "        if val == 'same_ans':\n",
    "            same += 1\n",
    "        elif val == 'no_temporal':\n",
    "            no_temp += 1\n",
    "        elif val == 'diff_ans':\n",
    "            diff += 1\n",
    "        elif val == 'no_direct':\n",
    "            no_dir +=1\n",
    "        else:\n",
    "            print(\"Invalid value\", val)\n",
    "\n",
    "print(\"No temporal loc in \", no_temp)\n",
    "print(\"Same ans as direct in \", same)\n",
    "print(\"Different ans than direct in\", diff)\n",
    "print(\"no direct in \", no_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../exports/%s/%s/%s/%s.txt' % ('dataset before 11-4', 'all', 'test', 'YSKX3')) as json_file:\n",
    "    QA = json.load(json_file)\n",
    "\n",
    "c = comp_test['YSKX3']\n",
    "cm = comp_test_mult['YSKX3']\n",
    "\n",
    "for q_id in QA:\n",
    "    if q_id in c:\n",
    "        tp = c[q_id]\n",
    "    elif q_id in cm:\n",
    "        tp = cm[q_id]\n",
    "    else:\n",
    "        tp = 'direct'\n",
    "        \n",
    "    print(q_id, tp, QA[q_id]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so if its not in it , its direect\n",
    "\n",
    "# well that's how it should be but its not - so double check\n",
    "\n",
    "\n",
    "# OK SO THEORETICALLY - I Could instead split it by time, not time. and then see. would be easy\n",
    "# can just make another funciton... and then don't run into this no direct problem ( i think) or would be a lot smaller\n",
    "# and hen just make everthing no_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = same_direct_te['YSKX3']['andObjRelExists']['v008-o4-o26'] #same_direct_te['YSKX3']['andObjRelExists']['r22-o4-o26']\n",
    "for i in dic: #[t_id][direct]\n",
    "    print(i)\n",
    "    for j in dic[i]:\n",
    "        print(\"   \", j)\n",
    "        if i == 'one':\n",
    "            for k in dic[i][j]:\n",
    "                print(\"     \", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small = getIndirectRefConsistencyTime(d_path, 'test', 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "if False:\n",
    "    print(\"test_one\", len(diff_pairs_test), len(same_pairs_test), len(no_direct_test))\n",
    "    print(\"test_mult\", len(diff_pairs_test_mult), len(same_pairs_test_mult), len(no_direct_test_mult))\n",
    "    #print(\"train\", len(diff_pairs_train), len(same_pairs_train), len(no_direct_train))\n",
    "    #\n",
    "    # NOTE! This doesnt involve ones with two where the temporal localization does nothing, but im not sure how to do that. Maybe if different from direct and it has a temporal localizor\n",
    "\n",
    "    same_pairs_test_all = same_pairs_test + same_pairs_test_mult\n",
    "    print(len(same_pairs_test_all))\n",
    "\n",
    "\n",
    "if False:\n",
    "    with open(\"../AMT/easyturk/easyturk/data/same_pairs_test.txt\", 'w+') as outfile:\n",
    "        json.dump(same_pairs_test_all, outfile)\n",
    "\n",
    "\n",
    "if False: \n",
    "    for i in diff_pairs:\n",
    "        with open('../exports/dataset/test/%s.txt' % i[0][:5]) as json_file:\n",
    "                QA = json.load(json_file)\n",
    "        print()\n",
    "        print(QA[i[0]]['answer'], QA[i[0]]['question'])\n",
    "        print(QA[i[1]]['answer'], QA[i[1]]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp_g_tr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in steps_glob_sp_te['less']['T5ECU']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_tr, t_tr = splitCompoStepsByGlobTempl('dataset', 'train', 'templ_balanced')\n",
    "g_te, t_te = splitCompoStepsByGlobTempl('dataset', 'test', 'templ_balanced')\n",
    "sp_g_tr_idx, sp_g_tr, sp_g_tr_templ = findGlobalSplit(g_tr, t_tr, .5)\n",
    "sp_g_te_idx, sp_g_te, sp_g_te_templ = findGlobalSplit(g_te, t_te, .5)\n",
    "sp_t_tr_idx, sp_t_tr, _ = findGlobalSplit(t_tr, None, .5)\n",
    "sp_t_te_idx, sp_t_te, _ = findGlobalSplit(t_te, None, .5)\n",
    "\n",
    "steps_glob_sp_tr = splitQIDS(sp_g_tr_idx, steps_tr_b_glob)\n",
    "steps_glob_sp_te = splitQIDS(sp_g_te_idx, steps_te_b_glob)\n",
    "steps_templ_sp_tr = splitQIDS(sp_t_tr_idx, steps_tr_b_templ)\n",
    "steps_templ_sp_te = splitQIDS(sp_t_te_idx, steps_te_b_templ)\n",
    "\n",
    "#dumpMetrics(steps_glob_sp_tr, \"steps_by_glob\", d_path, 'train', balanced)\n",
    "#dumpMetrics(steps_glob_sp_te, \"steps_by_glob\", d_path, 'test', balanced)\n",
    "dumpMetrics(steps_templ_sp_tr, \"steps_by_templ\", d_path, 'train', balanced)\n",
    "dumpMetrics(steps_templ_sp_te, \"steps_by_templ\", d_path, 'test', balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Numbers on Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "illegal_comp_te_b = loadMetrics(illegal_comp_title, d_path, 'test', balanced)\n",
    "illegal_comp_tr_b = loadMetrics(illegal_comp_title, d_path, 'train', balanced)\n",
    "#illegal_comp_te_a = loadMetrics(illegal_comp_title, d_path, 'test', balanced=False)\n",
    "#illegal_comp_tr_a = loadMetrics(illegal_comp_title, d_path, 'train', balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for glob in illegal_comp_tr_b:\n",
    "    for q_id in illegal_comp_tr_b[glob]:\n",
    "        if q_id[:5] == '46GP8':\n",
    "            print(q_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"TRAIN all\")\n",
    "#for i in illegal_comp_tr_a:\n",
    "#    print(i, len(illegal_comp_tr_a[i]))\n",
    "print()\n",
    "print(\"TEST all\")\n",
    "#for i in illegal_comp_te_a:\n",
    "#    print(i, len(illegal_comp_te_a[i]))\n",
    "print()\n",
    "print(\"TRAIN balanced\")\n",
    "for i in illegal_comp_tr_b:\n",
    "    print(i, len(illegal_comp_tr_b[i]))\n",
    "print()\n",
    "print(\"TEST balanced\")\n",
    "for i in illegal_comp_te_b:\n",
    "    print(i, len(illegal_comp_te_b[i]), illegal_comp_te_b[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(illegal_comp_te_b['before'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dumpMetrics(comp_by_vid_train, illegal_comp_title + \"-by-vid\", d_path, 'train', balanced)\n",
    "#dumpMetrics(comp_by_vid_test, illegal_comp_title + \"-by-vid\", d_path, 'test', balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for v in comp_by_vid_test:\n",
    "    total = total + len(comp_by_vid_test[v])\n",
    "    \n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_by_vid['YSKX3'], len(comp_by_vid['YSKX3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = pickleLoad('../data/IDX.pkl')\n",
    "PP = pickleLoad('../data/pres_part.pkl')\n",
    "PRES = pickleLoad('../data/present.pkl')\n",
    "PAST = pickleLoad('../data/past.pkl')\n",
    "import grammar as g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE_COMP = [\n",
    "        IDX[\"standing up\"],\n",
    "        IDX[\"playing with a phone\"],\n",
    "        IDX[\"throwing a broom somewhere\"],\n",
    "        #IDX[\"holding a blanket\"],\n",
    "    ]\n",
    "\n",
    "FIRST_COMP = [\n",
    "        IDX['looking_at'],\n",
    "        IDX['behind'],\n",
    "        IDX['holding'],\n",
    "    ]\n",
    "\n",
    "LONGER_COMP = [\n",
    "        IDX[\"standing up\"],\n",
    "        IDX[\"playing with a phone\"],\n",
    "        IDX[\"throwing a broom somewhere\"],\n",
    "        #IDX[\"holding a blanket\"],\n",
    "    ]\n",
    "\n",
    "REPETITION_COMP = [\n",
    "        IDX['taking a cup from somewhere'],\n",
    "        IDX['putting some food somewhere'],\n",
    "        #IDX[\"holding a blanket\"],\n",
    "    ]\n",
    "\n",
    "# THese cannot be in actions\n",
    "OBJREL_COMP = [\n",
    "        (IDX['table'], IDX['wiping']), #TODO: these need to be changed b/c touching for a lot of other things. needs to be more distinct verbs\n",
    "        (IDX['cup'], IDX['wiping']),\n",
    "        (IDX['table'], IDX['beneath']),\n",
    "        (IDX['cup'], IDX['beneath']),\n",
    "        (IDX['food'], IDX['in front of']),\n",
    "        (IDX['book'], IDX['carrying']),\n",
    "        (IDX['chair'], IDX['leaning on']),\n",
    "        #(IDX['table'], IDX['touching']), #TODO: these need to be changed b/c touching for a lot of other things. needs to be more distinct verbs\n",
    "        #(IDX['food'], IDX['touching']),\n",
    "        #(IDX['table'], IDX['beneath']),\n",
    "        #(IDX['food'], IDX['beneath']),\n",
    "       # (IDX['blanket'], IDX['holding']),\n",
    "       # (IDX['blanket'], IDX['carrying']),\n",
    "    ]\n",
    "\n",
    "NOVEL_COMPS = {\n",
    "    'before': BEFORE_COMP,\n",
    "    'first': FIRST_COMP,\n",
    "    'longer': LONGER_COMP,\n",
    "    'repetition': REPETITION_COMP,\n",
    "    'objrel': OBJREL_COMP\n",
    "}\n",
    "\n",
    "def substringSearch(q_id, q, ncomp):\n",
    "    question = q['question']\n",
    "    cats = set()\n",
    "    if ncomp == 'before': \n",
    "        for act in NOVEL_COMPS[ncomp]:\n",
    "            phrases = [\n",
    "                \"before starting to %s\" % PRES[act],\n",
    "                \"before %s\" % PP[act]\n",
    "            ]\n",
    "            for phrase in phrases:\n",
    "                if question.find(phrase) != -1:\n",
    "                    cats.add(act)\n",
    "       # \"before starting to %s\" % (PRES[indirect['direct']]), # TODO can't think of any....\n",
    "        #                \" before %s\" % (indirect['phrase']), # TODO can't think of any....\n",
    "                    \n",
    "    if ncomp =='first':\n",
    "        for rel in NOVEL_COMPS[ncomp]:\n",
    "            phrases = [\n",
    "                \"%s first\" % PP[rel],\n",
    "                \"first thing they %s\" % PAST[rel]\n",
    "            ]\n",
    "            for phrase in phrases:\n",
    "                if question.find(phrase) != -1:\n",
    "                    cats.add(rel)\n",
    "        \n",
    "        # for eveything in before comp, look for phrae\n",
    "        #\"the object they were %s first%s\" % (g.PP[r], indirect['phrase']),\n",
    "        #            \"the first thing they %s%s\" % (PAST[r], indirect['phrase']), # TODO: can add with past tense later\n",
    "         # can simplfy. \"firt hing they %s\" and \"%s first\"  \n",
    "    if ncomp == 'longer':\n",
    "        print()\n",
    "        print(\"WILL BE more complicated than i thought b/c have to connect to object.... but it might be in indirect object ref... \")\n",
    "        print(\"Indirect longer reference b/c in object ref\")\n",
    "        print(q_id, question)\n",
    "        print()\n",
    "        #for rel in NOVEL_COMPS[ncomp]:\n",
    "        \n",
    "        #    phrases = [\n",
    "        #        \"%s first\" % PP[rel],\n",
    "        #        \"first thing they %s\" % PAST[r]\n",
    "        #    ]\n",
    "        #    for phrase in phrases:\n",
    "        #        if question.find(phrase) != -1:\n",
    "        #            cats.add(act)\n",
    "        \n",
    "        #phrases = [\n",
    "        #    \"doing the action they did the longest\",\n",
    "        #    \"doing the longest action\",\n",
    "        #    \"doing the thing they spent the longest amount of time doing\",\n",
    "        #]\n",
    "        # shoudlnt need this... b/c so\n",
    "    if ncomp == 'repetition':\n",
    "        print(\"Shouldn't have missed a repetition one\", q_id, q['question'])\n",
    "        \n",
    "    if ncomp == 'objrel':\n",
    "        # will be more complicated b/c need to connect refs with the words\n",
    "        print(\"in obj rel\", q_id, q['question'])\n",
    "    return cats\n",
    "\n",
    "\n",
    "def divideByComps(q, q_id, divided, ncomp):\n",
    "    lst = NOVEL_COMPS[ncomp]\n",
    "    \n",
    "    # get list of dic of all ones in includes\n",
    "    args = q['metrics']['direct_args'].split(\"-\") + q['metrics']['direct_time'][1].split(\"-\")\n",
    "    cats = set()\n",
    "    for arg in args:\n",
    "        if arg == \"\":\n",
    "            continue\n",
    "        if arg in lst:\n",
    "            cats.add(arg)\n",
    "            \n",
    "    if not cats:\n",
    "        #print(\"cats is empty\")\n",
    "        cats = substringSearch(q_id, q, ncomp)\n",
    "    # add q_id to divdeded (adding in arg to dic if needed)\n",
    "    for arg in cats:\n",
    "        if arg not in divided[ncomp]:\n",
    "            divided[ncomp][arg] = []\n",
    "        divided[ncomp][arg].append(q_id)\n",
    "    # return\n",
    "    return divided\n",
    "    \n",
    "def findWhichCompObjRel(q, q_id, divided, ncomp):\n",
    "    lst = NOVEL_COMPS[ncomp]\n",
    "    \n",
    "    # get list of dic of all ones in includes\n",
    "    args = q['metrics']['direct_args'].split(\"-\") + q['metrics']['direct_time'][1].split(\"-\")\n",
    "    cats = set()\n",
    "    objs = set()\n",
    "    rels = set()\n",
    "    \n",
    "    for arg in args:\n",
    "        if arg == \"\":\n",
    "            continue\n",
    "        tp = g.vType(arg)\n",
    "        if tp == 'objects':\n",
    "            objs.add(arg)\n",
    "        if tp in ['attention', 'spatial', 'contact', 'verb']:\n",
    "            rels.add(arg)\n",
    "        \n",
    "    \n",
    "    if not objs or not rels:\n",
    "        print(\"THis should not be happnening\")\n",
    "    \n",
    "    for obj in objs:\n",
    "        for rel in rels:\n",
    "            if (obj, rel) in lst:\n",
    "                cats.add((obj, rel))\n",
    "                \n",
    "    if not cats: \n",
    "        print('CATS IS EMPTY FOR THIS ONE')\n",
    "            \n",
    "    # add q_id to divdeded (adding in arg to dic if needed)\n",
    "    for arg in cats:\n",
    "        if arg not in divided[ncomp]:\n",
    "            divided[ncomp][arg] = []\n",
    "        divided[ncomp][arg].append(q_id)\n",
    "    # return\n",
    "    return divided\n",
    "    \n",
    "\n",
    "def splitNovelComp(d_path, group, balanced, ncomps):\n",
    "    divided = {}\n",
    "    \n",
    "    for ncomp in ncomps:\n",
    "        print(ncomp, len(ncomps[ncomp]))\n",
    "        divided[ncomp] = {}\n",
    "        count = 0\n",
    "        \n",
    "        QA = {}\n",
    "        prev_vid = -1\n",
    "        for q_id in ncomps[ncomp]:\n",
    "            \n",
    "            v_id = q_id[:5]\n",
    "            if v_id != prev_vid:\n",
    "                QA = getVideoQA(d_path, group, balanced, v_id)\n",
    "\n",
    "            q = QA[q_id]\n",
    "            #print(q_id, q['question'])\n",
    "            \n",
    "            if ncomp == 'objrel':\n",
    "                divided = findWhichCompObjRel(q, q_id, divided, ncomp)\n",
    "            else:\n",
    "                divided = divideByComps(q, q_id, divided, ncomp)\n",
    "                \n",
    "            \n",
    "            prev_vid = v_id\n",
    "            if count % 10000 == 0:\n",
    "                print(\" \"*4, count)\n",
    "            count = count + 1\n",
    "            \n",
    "            if count == 100000:\n",
    "                break\n",
    "    return divided\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = 'test'\n",
    "novel_comp_tr_b = []\n",
    "no_cat_tr_b = []\n",
    "ranges = [(0, 100000), (100000, 200000), (200000, 400000), (400000, 600000), (600000, 900000), (900000, 1200000)]\n",
    "        \n",
    "for start, stop in ranges:\n",
    "    novel_comp_tr_b.append(pickleLoad('../exports/%s/metrics/balanced/%s-%s-%s.txt' % (d_path, group, start, stop)))\n",
    "    #with open('../exports/%s/metrics/balanced/%s-%s-%s.txt' % (d_path, group, start, stop), 'rb') as json_file:\n",
    "    #    novel_comp_tr_b.append(json.load(json_file))\n",
    "    no_cat_tr_b.append(pickleLoad('../exports/%s/metrics/balanced/%s-nocat-%s-%s.txt' % (d_path, group, start, stop)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncomp in divided_te_b:\n",
    "    #if ncomp != 'before':\n",
    "    #    continue\n",
    "    print(ncomp)\n",
    "    for spec in divided_te_b[ncomp]:\n",
    "        print(\" \"*4, spec, len(divided_te_b[ncomp][spec]))\n",
    "        \n",
    "        #print(divided_te_b[ncomp][spec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"YSKX3-466\": {\"question\": \"Before putting a pillow somewhere, did they touch the first thing they went behind?\", \"answer\": \"Yes\", \"attributes\": {\"type\": \"objRelExists\", \"structural\": \"verify\", \"semantic\": \"objrel\", \"ans_type\": \"binary\", \"video_id\": \"YSKX3\"}, \"global\": \"exists\", \"local\": \"r22-o4\", \"steps\": 4, \"id\": \"YSKX3-466\", \"metrics\": {\"novel_comp\": [false, true, false, false, false], \"indirects\": [true, false, false, true], \"direct_args\": \"r22-o4\", \"direct_time\": [\"before\", \"c077\"]}}, \n",
    "\n",
    "\n",
    " \"T5ECU-2002\": {\"question\": \"Before starting to hold a vacuum, did they lean on a chair?\", \"answer\": \"No\", \"attributes\": {\"type\": \"objRelExists\", \"structural\": \"verify\", \"semantic\": \"objrel\", \"ans_type\": \"binary\", \"video_id\": \"T5ECU\"}, \"global\": \"exists\", \"local\": \"r16-o8\", \"steps\": 3, \"id\": \"T5ECU-2002\", \"metrics\": {\"novel_comp\": [false, false, false, false, true], \"indirects\": [false, false, false, true], \"direct_args\": \"r16-o8\", \"direct_time\": [\"before\", \"c137\"]}}, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "ans_type_te_b = loadMetrics(ans_type_title, d_path, 'test', balanced=True)\n",
    "ans_type_tr_b = loadMetrics(ans_type_title, d_path, 'train', balanced=True)\n",
    "ans_type_te_a = loadMetrics(ans_type_title, d_path, 'test', balanced=False)\n",
    "ans_type_tr_a = loadMetrics(ans_type_title, d_path, 'train', balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"TRAIN all\")\n",
    "for i in ans_type_tr_a:\n",
    "    print(i, len(ans_type_tr_a[i]))\n",
    "print()\n",
    "print(\"TEST all\")\n",
    "for i in ans_type_te_a:\n",
    "    print(i, len(ans_type_te_a[i]))\n",
    "print()\n",
    "print(\"TRAIN balanced\")\n",
    "for i in ans_type_tr_b:\n",
    "    print(i, len(ans_type_tr_b[i]))\n",
    "print()\n",
    "print(\"TEST balanced\")\n",
    "for i in ans_type_te_b:\n",
    "    print(i, len(ans_type_te_b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "balanced = 'balanced'\n",
    "a = 'all'\n",
    "global_te_b = loadMetrics(templ_title, d_path, 'test', balanced)\n",
    "global_tr_b = loadMetrics(templ_title, d_path, 'train', balanced)\n",
    "global_te_a = loadMetrics(templ_title, d_path, 'test', a)\n",
    "global_tr_a = loadMetrics(templ_title, d_path, 'train', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print()\n",
    "    print(\"TRAIN all\")\n",
    "    for i in global_tr_a:\n",
    "        print(i, len(global_tr_a[i]))\n",
    "    print()\n",
    "    print(\"TEST all\")\n",
    "    for i in global_te_a:\n",
    "        print(i, len(global_te_a[i]))\n",
    "    print()\n",
    "print(\"TOTAL all\")\n",
    "for i in global_te_a:\n",
    "    #print(i, len(global_te_a[i]) + len(global_tr_a[i]))\n",
    "    print(i, (global_te_a[i] + global_tr_a[i]) / 1000)\n",
    "\n",
    "if False:    \n",
    "    print()\n",
    "    print(\"TRAIN balanced\")\n",
    "    for i in global_tr_b:\n",
    "        print(i, len(global_tr_b[i]))\n",
    "    print()\n",
    "    print(\"TEST balanced\")\n",
    "    for i in global_te_b:\n",
    "        print(i, len(global_te_b[i]))\n",
    "print()\n",
    "print(\"TOTAL balanced\")\n",
    "for i in global_te_a:\n",
    "    #print(i, len(global_te_b[i]) + len(global_tr_b[i]))\n",
    "    print(i, (global_te_b[i] + global_tr_b[i]) / 1000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = 'dataset'\n",
    "same_direct_te_b = loadMetrics(same_direct_title, d_path, 'test', balanced=True)\n",
    "same_direct_tr_b = loadMetrics(same_direct_title, d_path, 'train', balanced=True)\n",
    "same_direct_te_a = loadMetrics(same_direct_title, d_path, 'test', balanced=False)\n",
    "same_direct_tr_a = loadMetrics(same_direct_title, d_path, 'train', balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"TRAIN all\")\n",
    "#for i in same_direct_tr_a:\n",
    "#    print(i, len(same_direct_tr_a[i]))\n",
    "print()\n",
    "print(\"TEST all\")\n",
    "#for i in same_direct_te_a:\n",
    "#    print(i, len(same_direct_te_a[i]))\n",
    "print()\n",
    "print(\"TRAIN balanced\")\n",
    "for i in same_direct_tr_b:\n",
    "    print(i, len(same_direct_tr_b[i]))\n",
    "print()\n",
    "print(\"TEST balanced\")\n",
    "for i in same_direct_te_b:\n",
    "    print(i, len(same_direct_te_b[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndirectRefSummary(dic):\n",
    "    # its video id -> tempalte -> local -> direct args # all/direct/one/two. in one is object, relation, action, time\n",
    "    # I want: # direct\n",
    "    # average # indirect per direct\n",
    "    # average # of all categories, so 1, relation, tim etc\n",
    "    # SO i need: total # direct, total # all (and subtract direct to get avg indirect per direct)\n",
    "    # total number for each one , two, etc\n",
    "    # \n",
    "    \n",
    "    nums = {\n",
    "        'all': 0,\n",
    "        'direct': 0,\n",
    "        'one-time': 0,\n",
    "        'one-object': 0,\n",
    "        'one-relation': 0,\n",
    "        'one-action': 0,\n",
    "        'two': 0\n",
    "    }\n",
    "    \n",
    "    for v_id in dic:\n",
    "        for t_id in dic[v_id]:\n",
    "            for direct in dic[v_id][t_id]:\n",
    "                d = dic[v_id][t_id][direct]\n",
    "                nums['all'] = nums['all'] + len(d['all'])\n",
    "                nums['direct'] = nums['direct'] + len(d['direct'])\n",
    "                nums['one-time'] = nums['one-time'] + len(d['one']['time'])\n",
    "                nums['one-object'] = nums['one-object'] + len(d['one']['object'])\n",
    "                nums['one-relation'] = nums['one-relation'] + len(d['one']['relation'])\n",
    "                nums['one-action'] = nums['one-action'] + len(d['one']['action'])\n",
    "                nums['two'] = nums['two'] + len(['two'])\n",
    "    return nums\n",
    "\n",
    "print(\"TRAIN all\")\n",
    "#ir_summary_tr_a = getIndirectRefSummary(same_direct_tr_a)\n",
    "ir_summary_tr_b = getIndirectRefSummary(same_direct_tr_b)\n",
    "#ir_summary_te_a = getIndirectRefSummary(same_direct_te_a)\n",
    "ir_summary_te_b = getIndirectRefSummary(same_direct_te_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN all\")\n",
    "#print(ir_summary_tr_a)\n",
    "print()\n",
    "print(\"TEST all\")\n",
    "#print(ir_summary_te_a)\n",
    "print()\n",
    "print(\"TRAIN bal\")\n",
    "print(ir_summary_tr_b)\n",
    "print()\n",
    "print(\"TEST bal\")\n",
    "print(ir_summary_te_b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find percent with 1 and % with 2\n",
    "\n",
    "for i in ir_summary_te_b:\n",
    "    num = ir_summary_te_b[i]\n",
    "    print(i, round(num / ir_summary_te_b['all'] * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ir_summary_te_b['all'] - ir_summary_te_b['direct'])# / ir_summary_te_b['direct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_tr, t_tr = splitCompoStepsByGlobTempl('dataset', 'train', 'templ_balanced')\n",
    "#g_te, t_te = splitCompoStepsByGlobTempl('dataset', 'test', 'templ_balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp5_g_tr = findUniversalSplit(g_tr, 5)\n",
    "sp5_t_tr = findUniversalSplit(t_tr, 5)\n",
    "sp5_g_te = findUniversalSplit(g_te, 5)\n",
    "sp5_t_te = findUniversalSplit(t_te, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_tr, t_tr = splitCompoStepsByGlobTempl('dataset', 'train', 'templ_balanced')\n",
    "#g_te, t_te = splitCompoStepsByGlobTempl('dataset', 'test', 'templ_balanced')\n",
    "sp_g_tr_idx, sp_g_tr, sp_g_tr_templ = findGlobalSplit(g_tr, t_tr, .5)\n",
    "sp_g_te_idx, sp_g_te, sp_g_te_templ = findGlobalSplit(g_te, t_te, .5)\n",
    "sp_t_tr_idx, sp_t_tr, _ = findGlobalSplit(t_tr, None, .5)\n",
    "sp_t_te_idx, sp_t_te, _ = findGlobalSplit(t_te, None, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ''\n",
    "for i in sp_g_tr_idx:\n",
    "    new_str = '%s: %s, ' % (i, sp_g_tr_idx[i])\n",
    "    s += new_str\n",
    "    \n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def alphabetize(z):\n",
    "    sort_z = sorted(z, key=lambda x: x[0])\n",
    "    x = []\n",
    "    y = []\n",
    "    for a, b in sort_z:\n",
    "        x.append(a)\n",
    "        y.append(b)\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "\n",
    "def visSplit(dic, split, title):\n",
    "    # 1- distribution of templates in training/test\n",
    "    z = []\n",
    "    \n",
    "    size = 8\n",
    "    plt.subplots(figsize=(size, size * .75))\n",
    "    \n",
    "    for i in dic: \n",
    "        z.append((i, dic[i][split]))\n",
    "        \n",
    "    \n",
    "    if split == 'less':\n",
    "        plt.title(title + \" Training\")\n",
    "        x, y = alphabetize(z)\n",
    "        sns.barplot(y, x)\n",
    "        plt.show()\n",
    "    \n",
    "    if split == 'more':\n",
    "        plt.title(title + \" Testing\")\n",
    "        x, y = alphabetize(z)\n",
    "        sns.barplot(y, x)\n",
    "        plt.show()\n",
    "    \n",
    "    # 2- proportion of each template in each\n",
    "#visSplit(sp5_g_tr, \"less\", \"Global Split By 5 -\")\n",
    "#visSplit(sp5_t_tr, \"less\", \"Templ Split By 5 -\")\n",
    "#visSplit(sp5_g_te, \"more\", \"Global Split By 5 -\")\n",
    "#visSplit(sp5_t_te, \"more\", \"Templ Split By 5 -\")\n",
    "\n",
    "visSplit(sp_g_tr, \"less\", \"Global Split-\")\n",
    "visSplit(sp_g_tr_templ, \"less\", \"Global Split-\")\n",
    "visSplit(sp_g_te, \"more\", \"Global Split -\")\n",
    "visSplit(sp_g_te_templ, \"more\", \"Global Split -\")\n",
    "\n",
    "#visSplit(sp_t_tr, \"less\", \"Templ Split-\")\n",
    "#visSplit(sp_t_te, \"more\", \"Templ Split -\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Stuff Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalized saving\n",
    "# go thorugh all and save into new file\n",
    "# create a dict of title --> file paths so that easier in \n",
    "\n",
    "to_save = {\n",
    "    \"length\": [lengths_a, lengths_b],\n",
    "    \"global\": [global_a, global_b],\n",
    "    \"answer_type\": [answer_type_a, answer_type_b],\n",
    "    \"steps\": [steps_a, steps_b],\n",
    "    \"same_direct\": [same_direct_a, same_direct_b],\n",
    "    \"novel_compositions\": [illegal_comp_a, illegal_comp_b]\n",
    "}\n",
    "\n",
    "num_videos = 500\n",
    "balanced = \"bal\"\n",
    "all_vid = 'all'\n",
    "paths = {}\n",
    "\n",
    "for name in to_save:\n",
    "    path_a = \"../exports/metrics/%s_%s_%s.txt\" % (all_vid, num_videos, name)\n",
    "    path_b = \"../exports/metrics/%s_%s_%s.txt\" % (balanced, num_videos, name)\n",
    "    \n",
    "    if to_save[name][0] is None or to_save[name][1] is None:\n",
    "        print(name, \" is None!\")\n",
    "    with open(path_a, 'w+') as outfile:\n",
    "        json.dump(to_save[name][0], outfile)\n",
    "    \n",
    "    with open(path_b, 'w+') as outfile:\n",
    "        json.dump(to_save[name][1], outfile)\n",
    "    paths[name] = [path_a, path_b]\n",
    "        \n",
    "print(paths)\n",
    "\n",
    "with open(\"../exports/metrics/paths.txt\", 'w+') as outfile:\n",
    "    json.dump(paths, outfile)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them!\n",
    "\n",
    "with open(\"../exports/metrics/bal_500_length.txt\", 'w') as outfile:\n",
    "    json.dump(length_gen, outfile)\n",
    "    \n",
    "\n",
    "with open(\"../exports/metrics/bal_500_glob.txt\", 'w') as outfile:\n",
    "    json.dump(glob, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISSUES\n",
    "# 1- some things don't have somehting with all direct, even objLast which should... unless it's tied!\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Check lengths make sense for all and add up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaTotal()\n",
    "\n",
    "printLens(length_gen, \"Video Lengths\")\n",
    "printLens(glob, \"Global Variables\")\n",
    "printLens(answer_type, \"Answer Types\")\n",
    "printLens(steps, \"Number compositional Steps\")\n",
    "printLens(same_direct, \"Have same direct (may not work)\")\n",
    "printLens(illegal_comp, \"Illegal Compositions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODOOOOOO\n",
    "    # need! to change from \"touching\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
