{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No indirect: There are 4373 after and 4298 before questions\n",
      "Any indirect: There are 21592 after and 21591 before questions\n",
      "Only indir obj: There are 17979 after and 18004 before questions\n",
      "The other two selections have far too small sample sizes\n",
      "\n",
      "No indirect: Model makes 875 after and 7796 before predictions\n",
      "Any indirect: Model makes 13771 after and 29412 before predictions\n",
      "Only indir obj: Model makes 13513 after and 22470 before preds\n"
     ]
    }
   ],
   "source": [
    "# Let's first take stock of the situation\n",
    "print(f'No indirect: There are {445 + 3928} after and {430 + 3868} before questions')\n",
    "print(f'Any indirect: There are {11283 + 10309} after and {2488 + 19103} before questions')\n",
    "print(f'Only indir obj: There are {11339 + 6640} after and {2174 + 15830} before questions')\n",
    "print(f'The other two selections have far too small sample sizes')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'No indirect: Model makes {445 + 430} after and {3928 + 3868} before predictions')\n",
    "print(f'Any indirect: Model makes {11283 + 2488} after and {10309 + 19103} before predictions')\n",
    "print(f'Only indir obj: Model makes {11339 + 2174} after and {6640 + 15830} before preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two questions to address\n",
    "# Question 1: Within the training set used by these models, is there a bias that a blind model can exploit?\n",
    "# Question 2: Do the biases vary between models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: First load the relevant dataframes\n",
    "\n",
    "# PSAC\n",
    "psac_tr = pd.read_csv('/vision/u/momergul/PSAC_2/code_file/data/dataset_balanced/Train_frameqa_question-balanced.csv')\n",
    "\n",
    "# HME\n",
    "hme_tr = pd.read_csv('/vision/u/momergul/HME-VideoQA/gif-qa/data/dataset_balanced/Train_frameqa_question-balanced.csv')\n",
    "split = hme_tr.shape[0] - int(0.1 * hme_tr.shape[0])\n",
    "hme_tr = hme_tr.loc[:split - 1, :]\n",
    "\n",
    "# HCRN\n",
    "hcrn_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs/Train_frameqa_question-balanced.csv')\n",
    "split = int(0.9 * hcrn_tr.shape[0])\n",
    "while (hcrn_tr.loc[split - 1, 'vid_id'] == hcrn_tr.loc[split, 'vid_id']):\n",
    "    split = split + 1\n",
    "hcrn_tr = hcrn_tr.loc[:split-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For psac data\n",
      "Equal: 5817\n",
      "More Before: 34756\n",
      "Avg Before Delta: 1.9705656513949321\n",
      "More After: 34429\n",
      "Avg After Delta: 1.9655526992287917\n",
      "Only Before: 30849\n",
      "Only After: 30539\n",
      "\n",
      "For hme data\n",
      "Equal: 5817\n",
      "More Before: 34756\n",
      "Avg Before Delta: 1.9705656513949321\n",
      "More After: 34429\n",
      "Avg After Delta: 1.9655526992287917\n",
      "Only Before: 30849\n",
      "Only After: 30539\n",
      "\n",
      "For hcrn data\n",
      "Equal: 5817\n",
      "More Before: 34756\n",
      "Avg Before Delta: 1.9705656513949321\n",
      "More After: 34429\n",
      "Avg After Delta: 1.9655526992287917\n",
      "Only Before: 30849\n",
      "Only After: 30539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Check answer distribution per question for each\n",
    "from collections import Counter\n",
    "\n",
    "def check_biases(df, model_name):\n",
    "    df_binaries = df[(df['answer'] == 'before') | (df['answer'] == 'after')]\n",
    "    q_to_c = {}\n",
    "        \n",
    "    # Get mapping from question to number of before/after answers\n",
    "    for row in df_binaries.iterrows():\n",
    "        question, answer = row[1]['question'], row[1]['answer']\n",
    "        \n",
    "        if question not in q_to_c:\n",
    "            q_to_c[question] = Counter()\n",
    "        q_to_c[question][answer] += 1\n",
    "        \n",
    "    only_before = 0\n",
    "    only_after = 0\n",
    "    more_before = 0\n",
    "    more_after = 0\n",
    "    equal = 0\n",
    "    avg_before_diff = 0\n",
    "    avg_after_diff = 0\n",
    "    \n",
    "    for question, a_dict in q_to_c.items():\n",
    "        if len(a_dict) == 1 and 'before' in a_dict:\n",
    "            only_before += 1\n",
    "            more_before += 1\n",
    "        elif len(a_dict) == 1 and 'after' in a_dict:\n",
    "            only_after += 1\n",
    "            more_after += 1\n",
    "        elif a_dict['before'] == a_dict['after']:\n",
    "            equal += 1\n",
    "        elif a_dict['before'] > a_dict['after']:\n",
    "            more_before += 1\n",
    "            avg_before_diff += a_dict['before'] - a_dict['after']\n",
    "        else:\n",
    "            more_after += 1\n",
    "            avg_after_diff += a_dict['after'] - a_dict['before']\n",
    "            \n",
    "    print(f'For {model_name} data')\n",
    "    print(f'Equal: {equal}')\n",
    "    print(f'More Before: {more_before}')\n",
    "    print(f'Avg Before Delta: {avg_before_diff / (more_before - only_before)}')\n",
    "    print(f'More After: {more_after}')\n",
    "    print(f'Avg After Delta: {avg_after_diff / (more_after - only_after)}')\n",
    "    print(f'Only Before: {only_before}')\n",
    "    print(f'Only After: {only_after}')\n",
    "    print()\n",
    "    \n",
    "        \n",
    "check_biases(psac_tr, 'psac')\n",
    "check_biases(hme_tr, 'hme')\n",
    "check_biases(hcrn_tr, 'hcrn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average before question length: 18.14688764331895\n",
      "Average after question length: 18.07037643207856\n"
     ]
    }
   ],
   "source": [
    "# Question 1: What about question length?\n",
    "lengths = {'before' : 0, 'after' : 0}\n",
    "answer_counts = Counter()\n",
    "psac_binaries = psac_tr[(psac_tr['answer'] == 'before') | (psac_tr['answer'] == 'after')]\n",
    "\n",
    "for row in psac_binaries.iterrows():\n",
    "    question, answer = row[1]['question'], row[1]['answer']\n",
    "    sentence = question.lower()\n",
    "    sentence = sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s')\n",
    "    words = sentence.split()    \n",
    "    lengths[answer] += len(words)\n",
    "    answer_counts[answer] += 1\n",
    "    \n",
    "print(f'Average before question length: {lengths[\"before\"] / answer_counts[\"before\"]}')\n",
    "print(f'Average after question length: {lengths[\"after\"] / answer_counts[\"after\"]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: Do biases vary between models? Load pred jsons\n",
    "import json\n",
    "\n",
    "# PSAC\n",
    "with open('/vision/u/momergul/PSAC_2/code_file/data_blind/prediction/FrameQA_prediction.json', 'r') as f:\n",
    "    psac_preds = json.load(f)\n",
    "    \n",
    "# HCRN\n",
    "with open('/vision/u/momergul/hcrn-videoqa_2/results/expTGIF-QAFrameQA_blind/preds/test_preds.json', 'r') as f:\n",
    "    hcrn_preds = json.load(f)\n",
    "    \n",
    "# HME \n",
    "with open('/vision/u/momergul/HME-VideoQA/gif-qa/data/prediction_balanced_blind/FrameQA_test_results-0-csv1.json', 'r') as f:\n",
    "    hme_preds = json.load(f)\n",
    "with open('/vision/u/momergul/HME-VideoQA/gif-qa/data/prediction_balanced_blind/FrameQA_test_results-0-csv2.json', 'r') as f:\n",
    "    hme_preds += json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For psac: Counter({'after': 24104, 'before': 20742})\n",
      "For hme: Counter({'before': 25219, 'after': 19617})\n",
      "For hcrn: Counter({'before': 30427, 'after': 14410})\n"
     ]
    }
   ],
   "source": [
    "def get_answer_counts(preds, model_name):\n",
    "    answer_counts = Counter()\n",
    "    \n",
    "    for pred in preds:\n",
    "        answer = pred['prediction']\n",
    "        if answer in ['before', 'after']:\n",
    "            answer_counts[answer] += 1\n",
    "            \n",
    "    print(f'For {model_name}: {answer_counts}')\n",
    "    \n",
    "get_answer_counts(psac_preds, 'psac')\n",
    "get_answer_counts(hme_preds, 'hme')\n",
    "get_answer_counts(hcrn_preds, 'hcrn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minor point: Do we represent all videos in the training set?\n",
    "df_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa/csvs_old/Train_frameqa_question-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7787"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_ids = set(vid_id[:5] for vid_id in list(df_tr['vid_id'].values))\n",
    "len(vid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_vid_ids = os.listdir('/vision/u/momergul/AGQA/exports/dataset/balanced/train')\n",
    "for vid_id in train_vid_ids:\n",
    "    if vid_id[:5] not in vid_ids:\n",
    "        print(vid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Figuring out the answer distributions on the test set\n",
    "test_df = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs/Test_frameqa_question-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data\n",
      "Equal: 2007\n",
      "More Before: 15274\n",
      "Avg Before Delta: 1.4421669106881405\n",
      "More After: 15469\n",
      "Avg After Delta: 1.436734693877551\n",
      "Only Before: 14591\n",
      "Only After: 14734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_biases(df, model_name, report_all=False):\n",
    "    df_binaries = df[(df['answer'] == 'before') | (df['answer'] == 'after')]\n",
    "    q_to_c = {}\n",
    "        \n",
    "    # Get mapping from question to number of before/after answers\n",
    "    for row in df_binaries.iterrows():\n",
    "        question, answer = row[1]['question'], row[1]['answer']\n",
    "        sentence = question.lower()\n",
    "        sentence = sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s')\n",
    "        \n",
    "        if sentence not in q_to_c:\n",
    "            q_to_c[sentence] = Counter()\n",
    "        q_to_c[sentence][answer] += 1\n",
    "        \n",
    "    only_before = 0\n",
    "    only_after = 0\n",
    "    more_before = 0\n",
    "    more_after = 0\n",
    "    equal = 0\n",
    "    avg_before_diff = 0\n",
    "    avg_after_diff = 0\n",
    "    \n",
    "    question_dict = {'only_before' : [], 'only_after' : [], 'more_before' : [],\n",
    "                    'more_after' : [], 'equal' : []}\n",
    "    \n",
    "    for question, a_dict in q_to_c.items():\n",
    "        if len(a_dict) == 1 and 'before' in a_dict:\n",
    "            if report_all:\n",
    "                only_before += a_dict['before']\n",
    "                more_before += a_dict['before']\n",
    "            else:\n",
    "                only_before += 1\n",
    "                more_before += 1\n",
    "            question_dict['only_before'].append(question)\n",
    "        elif len(a_dict) == 1 and 'after' in a_dict:\n",
    "            if report_all:\n",
    "                only_after += a_dict['after']\n",
    "                more_after += a_dict['after']\n",
    "            else:\n",
    "                only_after += 1\n",
    "                more_after += 1\n",
    "            question_dict['only_after'].append(question)\n",
    "        elif a_dict['before'] == a_dict['after']:\n",
    "            if report_all:\n",
    "                equal += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                equal += 1\n",
    "            question_dict['equal'].append(question)\n",
    "        elif a_dict['before'] > a_dict['after']:\n",
    "            if report_all:\n",
    "                more_before += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                more_before += 1\n",
    "            avg_before_diff += a_dict['before'] - a_dict['after']\n",
    "            question_dict['more_before'].append(question)\n",
    "        else:\n",
    "            if report_all:\n",
    "                more_after += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                more_after += 1\n",
    "            avg_after_diff += a_dict['after'] - a_dict['before']            \n",
    "            question_dict['more_after'].append(question)\n",
    "            \n",
    "    print(f'For {model_name} data')\n",
    "    print(f'Equal: {equal}')\n",
    "    print(f'More Before: {more_before}')\n",
    "    print(f'Avg Before Delta: {avg_before_diff / (more_before - only_before)}')\n",
    "    print(f'More After: {more_after}')\n",
    "    print(f'Avg After Delta: {avg_after_diff / (more_after - only_after)}')\n",
    "    print(f'Only Before: {only_before}')\n",
    "    print(f'Only After: {only_after}')\n",
    "    print()\n",
    "    \n",
    "    return question_dict\n",
    "\n",
    "question_dict = check_biases(test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate before/after questions in preds\n",
    "def isolate_before_after(preds):\n",
    "    new_preds = []\n",
    "    for pred in preds:\n",
    "        if pred['answer'] in ['before', 'after']:\n",
    "            new_preds.append(pred)\n",
    "    return new_preds\n",
    "\n",
    "psac_subpreds = isolate_before_after(psac_preds)\n",
    "hme_subpreds = isolate_before_after(hme_preds)\n",
    "hcrn_subpreds = isolate_before_after(hcrn_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44836"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hcrn_subpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psac results\n",
      "Only before acc: 0.7231867504283267\n",
      "Only after acc: 0.7770514364355754\n",
      "Equal acc: 0.49910714285714286\n",
      "Equal counts: 2236, 4480\n",
      "More before acc: 0.5109237255653507\n",
      "More after acc: 0.5636168691922802\n",
      "hcrn results\n",
      "Only before acc: 0.8974871501998858\n",
      "Only after acc: 0.6177533115430931\n",
      "Equal acc: 0.5\n",
      "Equal counts: 2240, 4480\n",
      "More before acc: 0.6515906477577615\n",
      "More after acc: 0.42280200142959257\n",
      "This shouldn't be happening\n",
      "This shouldn't be happening\n",
      "This shouldn't be happening\n",
      "hme results\n",
      "Only before acc: 0.811411274201839\n",
      "Only after acc: 0.6973676664563858\n",
      "Equal acc: 0.5\n",
      "Equal counts: 2240, 4480\n",
      "More before acc: 0.556151782292066\n",
      "More after acc: 0.5114367405289493\n"
     ]
    }
   ],
   "source": [
    "# Report success\n",
    "def extract_question(question, model_name):\n",
    "    if model_name == 'psac':\n",
    "        sentence = question.lower()\n",
    "        sentence = sentence.replace(',', '').replace('?', '').replace('\\'s', ' \\'s')\n",
    "        return sentence\n",
    "    elif model_name == 'hcrn':\n",
    "        return ' '.join(question)\n",
    "    else:\n",
    "        new_question = []\n",
    "        for token in question:\n",
    "            if token == '\\\\?':\n",
    "                break\n",
    "            new_question.append(token)\n",
    "        return ' '.join(new_question)\n",
    "\n",
    "def report_subset_accuracy(question_dict, preds, model_name):\n",
    "    accuracies = {'only_before' : {'correct' : 0, 'total' : 0},\n",
    "                 'only_after' : {'correct' : 0, 'total' : 0},\n",
    "                 'equal' : {'correct' : 0, 'total' : 0},\n",
    "                 'more_before' : {'correct' : 0, 'total' : 0},\n",
    "                 'more_after' : {'correct' : 0, 'total' : 0}}\n",
    "    \n",
    "    for pred in preds:\n",
    "        question = extract_question(pred['question'], model_name)\n",
    "        correct = 1 if pred['answer'] == pred['prediction'] else 0\n",
    "        \n",
    "        if question in question_dict['only_before']:\n",
    "            accuracies['only_before']['total'] += 1\n",
    "            accuracies['only_before']['correct'] += correct\n",
    "        elif question in question_dict['only_after']:\n",
    "            accuracies['only_after']['total'] += 1\n",
    "            accuracies['only_after']['correct'] += correct\n",
    "        elif question in question_dict['equal']:\n",
    "            accuracies['equal']['total'] += 1\n",
    "            accuracies['equal']['correct'] += correct\n",
    "        elif question in question_dict['more_before']:\n",
    "            accuracies['more_before']['total'] += 1\n",
    "            accuracies['more_before']['correct'] += correct\n",
    "        elif question in question_dict['more_after']:\n",
    "            accuracies['more_after']['total'] += 1\n",
    "            accuracies['more_after']['correct'] += correct\n",
    "        else:\n",
    "            print(\"This shouldn't be happening\")\n",
    "            \n",
    "    print(f'{model_name} results')\n",
    "    print(f'Only before acc: {accuracies[\"only_before\"][\"correct\"] / accuracies[\"only_before\"][\"total\"]}')\n",
    "    print(f'Only after acc: {accuracies[\"only_after\"][\"correct\"] / accuracies[\"only_after\"][\"total\"]}')\n",
    "    print(f'Equal acc: {accuracies[\"equal\"][\"correct\"] / accuracies[\"equal\"][\"total\"]}')\n",
    "    print(f'Equal counts: {accuracies[\"equal\"][\"correct\"]}, {accuracies[\"equal\"][\"total\"]}')\n",
    "    print(f'More before acc: {accuracies[\"more_before\"][\"correct\"] / accuracies[\"more_before\"][\"total\"]}')\n",
    "    print(f'More after acc: {accuracies[\"more_after\"][\"correct\"] / accuracies[\"more_after\"][\"total\"]}')\n",
    "    \n",
    "report_subset_accuracy(question_dict, psac_subpreds, 'psac')\n",
    "report_subset_accuracy(question_dict, hcrn_subpreds, 'hcrn')\n",
    "report_subset_accuracy(question_dict, hme_subpreds, 'hme')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data\n",
      "Equal: 4480\n",
      "More Before: 20119\n",
      "Avg Before Delta: 0.37753928708317364\n",
      "More After: 20237\n",
      "Avg After Delta: 0.3774124374553252\n",
      "Only Before: 17510\n",
      "Only After: 17439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp = check_biases(test_df, 'test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data\n",
      "Equal: 5817\n",
      "More Before: 34756\n",
      "Avg Before Delta: 1.9705656513949321\n",
      "More After: 34429\n",
      "Avg After Delta: 1.9655526992287917\n",
      "Only Before: 30849\n",
      "Only After: 30539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Question 4: If a question appears in both train and test, is it in the same category usually?\n",
    "\n",
    "tr_question_dict = check_biases(psac_tr, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only before acc: 0.7174738120540458\n",
      "Only after acc: 0.7068219137184655\n",
      "Equal acc: 0.12430939226519337\n",
      "More before acc: 0.09465020576131687\n",
      "More after acc: 0.09561752988047809\n"
     ]
    }
   ],
   "source": [
    "repetitions = {'only_before' : {'correct' : 0, 'total' : 0},\n",
    "              'only_after' : {'correct' : 0, 'total' : 0},\n",
    "              'equal' : {'correct' : 0, 'total' : 0},\n",
    "              'more_before' : {'correct' : 0, 'total' : 0},\n",
    "              'more_after' : {'correct' : 0, 'total' : 0}}\n",
    "\n",
    "for category in ['only_before', 'only_after', 'equal', 'more_before', 'more_after']:\n",
    "    for question in tr_question_dict[category]:\n",
    "        appears = False\n",
    "        for cat in ['only_before', 'only_after', 'equal', 'more_before', 'more_after']:\n",
    "            if question in question_dict[cat]:\n",
    "                appears = True\n",
    "        if not appears:\n",
    "            continue\n",
    "        \n",
    "        repetitions[category]['total'] += 1\n",
    "        if question in question_dict[category]:\n",
    "            repetitions[category]['correct'] += 1\n",
    "\n",
    "print(f'Only before acc: {repetitions[\"only_before\"][\"correct\"] / repetitions[\"only_before\"][\"total\"]}')\n",
    "print(f'Only after acc: {repetitions[\"only_after\"][\"correct\"] / repetitions[\"only_after\"][\"total\"]}')\n",
    "print(f'Equal acc: {repetitions[\"equal\"][\"correct\"] / repetitions[\"equal\"][\"total\"]}')\n",
    "print(f'More before acc: {repetitions[\"more_before\"][\"correct\"] / repetitions[\"more_before\"][\"total\"]}')\n",
    "print(f'More after acc: {repetitions[\"more_after\"][\"correct\"] / repetitions[\"more_after\"][\"total\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only before acc: 0.5554119167939828\n",
      "Only after acc: 0.5607531975367125\n",
      "Equal acc: 0.16822429906542055\n",
      "More before acc: 0.3174846625766871\n",
      "More after acc: 0.3112391930835735\n"
     ]
    }
   ],
   "source": [
    "repetitions = {'only_before' : {'correct' : 0, 'total' : 0},\n",
    "              'only_after' : {'correct' : 0, 'total' : 0},\n",
    "              'equal' : {'correct' : 0, 'total' : 0},\n",
    "              'more_before' : {'correct' : 0, 'total' : 0},\n",
    "              'more_after' : {'correct' : 0, 'total' : 0}}\n",
    "\n",
    "for category in ['only_before', 'only_after', 'equal', 'more_before', 'more_after']:\n",
    "    for question in question_dict[category]:\n",
    "        appears = False\n",
    "        for cat in ['only_before', 'only_after', 'equal', 'more_before', 'more_after']:\n",
    "            if question in tr_question_dict[cat]:\n",
    "                appears = True\n",
    "        if not appears:\n",
    "            continue\n",
    "        \n",
    "        repetitions[category]['total'] += 1\n",
    "        if question in tr_question_dict[category]:\n",
    "            repetitions[category]['correct'] += 1\n",
    "\n",
    "print(f'Only before acc: {repetitions[\"only_before\"][\"correct\"] / repetitions[\"only_before\"][\"total\"]}')\n",
    "print(f'Only after acc: {repetitions[\"only_after\"][\"correct\"] / repetitions[\"only_after\"][\"total\"]}')\n",
    "print(f'Equal acc: {repetitions[\"equal\"][\"correct\"] / repetitions[\"equal\"][\"total\"]}')\n",
    "print(f'More before acc: {repetitions[\"more_before\"][\"correct\"] / repetitions[\"more_before\"][\"total\"]}')\n",
    "print(f'More after acc: {repetitions[\"more_after\"][\"correct\"] / repetitions[\"more_after\"][\"total\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For train data\n",
      "Equal: 12356\n",
      "More Before: 74142\n",
      "More After: 77650\n",
      "Only Before: 26564\n",
      "Only After: 26846\n",
      "\n",
      "For test data\n",
      "Equal: 5794\n",
      "More Before: 19436\n",
      "More After: 19606\n",
      "Only Before: 10099\n",
      "Only After: 10256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Repeat same analysis with programs\n",
    "def check_program_biases(mode, report_all=True):\n",
    "    q_to_c = {}\n",
    "    \n",
    "    if mode == 'train':\n",
    "        ranges = [(0, 1110), (1110, 2220), (2220, 3330), (3330, 4440),\n",
    "                  (4440, 5550), (5550, 6660), (6660, 7787)]\n",
    "    else:\n",
    "        ranges = [(0, 303), (303, 555), (555, 807), (807, 1059),\n",
    "                  (1059, 1311), (1311, 1563), (1563, 1814)]\n",
    "    \n",
    "    for r in ranges:\n",
    "        with open(f'/vision/u/momergul/AGQA/code/omer_additions/program_stats/{mode}_{r[0]}_{r[1]}.pkl', 'rb') as f:\n",
    "            curr_qtoc = pickle.load(f)\n",
    "        \n",
    "        for program, counts in curr_qtoc.items():\n",
    "            if program not in q_to_c:\n",
    "                q_to_c[program] = Counter()\n",
    "            for answer, count in counts.items():\n",
    "                q_to_c[program][answer] += count\n",
    "\n",
    "    only_before = 0\n",
    "    only_after = 0\n",
    "    more_before = 0\n",
    "    more_after = 0\n",
    "    equal = 0\n",
    "    avg_before_diff = 0\n",
    "    avg_after_diff = 0\n",
    "    \n",
    "    question_dict = {'only_before' : [], 'only_after' : [], 'more_before' : [],\n",
    "                    'more_after' : [], 'equal' : []}\n",
    "    \n",
    "    for question, a_dict in q_to_c.items():\n",
    "        if len(a_dict) == 1 and 'before' in a_dict:\n",
    "            if report_all:\n",
    "                only_before += a_dict['before']\n",
    "                more_before += a_dict['before']\n",
    "            else:\n",
    "                only_before += 1\n",
    "                more_before += 1\n",
    "            question_dict['only_before'].append(question)\n",
    "        elif len(a_dict) == 1 and 'after' in a_dict:\n",
    "            if report_all:\n",
    "                only_after += a_dict['after']\n",
    "                more_after += a_dict['after']\n",
    "            else:\n",
    "                only_after += 1\n",
    "                more_after += 1\n",
    "            question_dict['only_after'].append(question)\n",
    "        elif a_dict['before'] == a_dict['after']:\n",
    "            if report_all:\n",
    "                equal += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                equal += 1\n",
    "            question_dict['equal'].append(question)\n",
    "        elif a_dict['before'] > a_dict['after']:\n",
    "            if report_all:\n",
    "                more_before += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                more_before += 1\n",
    "            avg_before_diff += a_dict['before'] - a_dict['after']\n",
    "            question_dict['more_before'].append(question)\n",
    "        else:\n",
    "            if report_all:\n",
    "                more_after += a_dict['before'] + a_dict['after']\n",
    "            else:\n",
    "                more_after += 1\n",
    "            avg_after_diff += a_dict['after'] - a_dict['before']            \n",
    "            question_dict['more_after'].append(question)\n",
    "            \n",
    "    print(f'For {mode} data')\n",
    "    print(f'Equal: {equal}')\n",
    "    print(f'More Before: {more_before}')\n",
    "    if not report_all:\n",
    "        print(f'Avg Before Delta: {avg_before_diff / (more_before - only_before)}')\n",
    "    print(f'More After: {more_after}')\n",
    "    if not report_all:\n",
    "        print(f'Avg After Delta: {avg_after_diff / (more_after - only_after)}')\n",
    "    print(f'Only Before: {only_before}')\n",
    "    print(f'Only After: {only_after}')\n",
    "    print()\n",
    "\n",
    "tr_program_dict = check_program_biases('train', True)\n",
    "te_program_dict = check_program_biases('test', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/vision/u/momergul/AGQA/code/omer_additions/program_stats/tr_program_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(tr_program_dict, f)\n",
    "with open('/vision/u/momergul/AGQA/code/omer_additions/program_stats/te_program_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(te_program_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First save the entire concatenated dataset, without any removal shenanigans\n",
    "og_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs_old/Train_frameqa_question-balanced.csv')\n",
    "added_tr = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/additional-train-balanced-tgif.csv')\n",
    "tr_df = pd.concat([og_tr, added_tr])\n",
    "tr_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/agqa_too_point_oh/entire_train_combined.csv')\n",
    "\n",
    "og_te = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs_old/Test_frameqa_question-balanced.csv')\n",
    "added_te = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/additional-test-balanced-tgif.csv')\n",
    "te_df = pd.concat([og_te, added_te])\n",
    "te_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/agqa_too_point_oh/entire_test_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1718292"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/combined/train-balanced_combined-tgif.csv')\n",
    "tr_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the concatenated, sliced and sorted forms of the AGQA 2.0 datasets\n",
    "\n",
    "# TRAIN\n",
    "#og_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs_old/Train_frameqa_question-balanced.csv')\n",
    "#added_tr = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/additional-train-balanced-tgif.csv')\n",
    "#tr_df = pd.concat([og_tr, added_tr])\n",
    "tr_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/combined/train-balanced_combined-tgif.csv')\n",
    "tr_df = tr_df[(tr_df['answer'] == 'before') | (tr_df['answer'] == 'after')]\n",
    "tr_df = tr_df.sort_values(by=['vid_id'], ascending=True)\n",
    "tr_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/temp.csv')\n",
    "tr_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/temp.csv')\n",
    "\n",
    "# TEST\n",
    "#og_te = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs_old/Test_frameqa_question-balanced.csv')\n",
    "#added_te = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/additional-test-balanced-tgif.csv')\n",
    "#te_df = pd.concat([og_te, added_te])\n",
    "te_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/combined/test-balanced_combined-tgif.csv')\n",
    "te_df = te_df[(te_df['answer'] == 'before') | (te_df['answer'] == 'after')]\n",
    "te_df = te_df.sort_values(by=['vid_id'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'before': 109213, 'after': 109213})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "answers = list(tr_df['answer'].values)\n",
    "answer_counts = Counter()\n",
    "for answer in answers:\n",
    "    answer_counts[answer] += 1\n",
    "    \n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117392\n"
     ]
    }
   ],
   "source": [
    "# Remove problematic questions from train\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the problematic id files\n",
    "problematic_ids = []\n",
    "filenames = os.listdir('/vision/u/momergul/qdec_misc/february_qdec_data/more')\n",
    "for file in filenames:\n",
    "    with open(f'/vision/u/momergul/qdec_misc/february_qdec_data/more/{file}', 'r') as f:\n",
    "        problematic_ids += json.load(f)\n",
    "problematic_ids = set(problematic_ids)\n",
    "print(len(problematic_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218426\n"
     ]
    }
   ],
   "source": [
    "print(tr_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101063\n"
     ]
    }
   ],
   "source": [
    "# Remove from train\n",
    "tr_df.drop(tr_df[tr_df.key.isin(problematic_ids)].index, inplace=True)\n",
    "print(tr_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117425\n"
     ]
    }
   ],
   "source": [
    "print(218426 - 101001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new train and test sets\n",
    "tr_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/only_before_after/Train_frameqa_question-balanced.csv')\n",
    "te_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/only_before_after/Test_frameqa_question-balanced.csv')\n",
    "\n",
    "total_df = pd.concat([tr_df, te_df])\n",
    "total_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/only_before_after/Total_frameqa_question-balanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'before': 50543, 'after': 50520})\n"
     ]
    }
   ],
   "source": [
    "tr_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/only_before_after/Train_frameqa_question-balanced.csv')\n",
    "\n",
    "from collections import Counter\n",
    "answers = list(tr_df['answer'].values)\n",
    "answer_counts = Counter()\n",
    "for answer in answers:\n",
    "    answer_counts[answer] += 1\n",
    "    \n",
    "print(answer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(tr_df['key'].values)) - len(set(list(tr_df['key'].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#      repetition repetition repetition repetition repetition repetition repetition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no repetitions on the original: True\n",
      "There are no repetitions on the additions: True\n",
      "There are no repetitions on the combination: False\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Is there repetition on the original datasets?\n",
    "og_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs_old/Train_frameqa_question-balanced.csv')\n",
    "print(f'There are no repetitions on the original: {len(list(og_tr[\"key\"].values)) == len(set(list(og_tr[\"key\"].values)))}')\n",
    "\n",
    "added_tr = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/additional-train-balanced-tgif.csv')\n",
    "print(f'There are no repetitions on the additions: {len(list(added_tr[\"key\"].values)) == len(set(list(added_tr[\"key\"].values)))}')\n",
    "\n",
    "tr_df = pd.concat([og_tr, added_tr])\n",
    "tr_df = tr_df[(tr_df['answer'] == 'before') | (tr_df['answer'] == 'after')]\n",
    "tr_df = tr_df.sort_values(by=['vid_id'], ascending=True)\n",
    "print(f'There are no repetitions on the combination: {len(list(tr_df[\"key\"].values)) == len(set(list(tr_df[\"key\"].values)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repetitions on the combination: 68\n"
     ]
    }
   ],
   "source": [
    "# Question 2: Report the number of repetitions in the combo\n",
    "print(f'Number of repetitions on the combination: {len(list(tr_df[\"key\"].values)) - len(set(list(tr_df[\"key\"].values)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no repetitions in the ids: False\n",
      "Original length: 76312\n",
      "Set length: 76310\n"
     ]
    }
   ],
   "source": [
    "# Question 3: Are there repetitions in the problematic questions?\n",
    "import json, os\n",
    "\n",
    "problematic_ids = []\n",
    "filenames = os.listdir('/vision/u/momergul/qdec_misc/february_qdec_data/only')\n",
    "for file in filenames:\n",
    "    with open(f'/vision/u/momergul/qdec_misc/february_qdec_data/only/{file}', 'r') as f:\n",
    "        problematic_ids += json.load(f)\n",
    "        \n",
    "print(f\"There are no repetitions in the ids: {len(problematic_ids) == len(set(problematic_ids))}\")\n",
    "print(f'Original length: {len(problematic_ids)}')\n",
    "print(f'Set length: {len(set(problematic_ids))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no repetitions on the original: False\n",
      "Difference: 53\n"
     ]
    }
   ],
   "source": [
    "# Question 4: Are there repetitions in the final dataset?\n",
    "final_tr = pd.read_csv('/vision/u/momergul/hcrn-videoqa_2/csvs/Train_frameqa_question-balanced.csv')\n",
    "print(f'There are no repetitions on the original: {len(list(final_tr[\"key\"].values)) == len(set(list(final_tr[\"key\"].values)))}')\n",
    "print(f'Difference: {len(list(final_tr[\"key\"].values)) - len(set(list(final_tr[\"key\"].values)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76323"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_df.to_csv('/vision/u/momergul/qdec_misc/february_qdec_data/temp.csv')\n",
    "final_df = pd.read_csv('/vision/u/momergul/qdec_misc/february_qdec_data/temp.csv')\n",
    "final_df.drop(final_df[final_df.key.isin(problematic_ids)].index, inplace=True)\n",
    "\n",
    "tr_df.shape[0] - final_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(final_df['key'].values)) - len(set(list(final_df['key'].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
